{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsU02rQV48ut"
   },
   "source": [
    "# An Exploration of Language Prediction: A Simplified Model for Shakespearean Text Generation\n",
    "\n",
    "The recent advancements in large language models have captivated many, leading to curiosity about the underlying mechanisms of such sophisticated technology. In this context, we propose to explore a fundamental model aimed at predicting Shakespearean text, demonstrating the basic principles of language model training in a simplified manner.\n",
    "\n",
    "The model is constructed using the following architecture:\n",
    "\n",
    "- **Embedding Layer**: This layer initializes with the vocabulary size, indicating the range of unique words the model needs to recognize, and an output dimension of 150, designed to create a dense representation of word meanings in a 150-dimensional vector space. The input shape parameter is set based on the sequence length minus one, preparing the model to process sequences of text one word at a time.\n",
    "  \n",
    "- **LSTM Layer**: A Long Short-Term Memory (LSTM) layer with 150 units is employed, utilizing a 'tanh' activation function. This choice is due to LSTM's capability to remember and learn from long sequences of data, making it particularly suited for text prediction tasks where the context and order of words are crucial.\n",
    "\n",
    "- **Dense Layer**: The final layer is a Dense layer with a softmax activation function. It outputs a probability distribution over the vocabulary, indicating the model's prediction for the next word in the sequence.\n",
    "\n",
    "- **Optimizer**: The Adam optimizer is used with a learning rate of 0.0005, a modification from the default setting intended to mitigate the risk of exploding gradients, thus ensuring more stable training.\n",
    "\n",
    "The model is compiled with a loss function of 'sparse_categorical_crossentropy' and tracks accuracy as a performance metric. This configuration offers a balance between computational efficiency and the capacity to capture the nuanced structure of Shakespearean English.\n",
    "\n",
    "This experiment serves as an illustrative example of how even simplistic models can approach the task of language prediction, providing insights into the methodologies that underpin more advanced language processing systems. Through such exploration, we gain a deeper understanding of the challenges and potential of machine learning in linguistic applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1n2cPM05s3Fy"
   },
   "source": [
    "## Mount Google Drive into Colab and read our dataset\n",
    "\n",
    "As this model can take quite a while to train (many hours or days depending on your hardware), we will be running this on [Google Colab](https://colab.google/), where we can access quite a bit of cloud GPU time for relatively little money.\n",
    "\n",
    "We will be saving all results on Google Drive for long term storage, as sessions do not keep any results when they are shutdown.\n",
    "\n",
    "As the dataset, we will be using [~100,000 lines from Shakespeare plays](https://www.kaggle.com/datasets/kingburrito666/shakespeare-plays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "UTZxz9aCs1Or",
    "outputId": "193cea59-1c07-4fe8-b509-1a22dc3b24d4",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true,
    "id": "9BfFECsUrQke",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(s):\n",
    "    return re.sub(r'[^a-zA-Z ]', '', s).lower()\n",
    "\n",
    "# Read all lines from alllines.txt which are in quotes, and add to an array.\n",
    "def read_lines():\n",
    "    lines = []\n",
    "    with open('/content/drive/MyDrive/Colab Notebooks/alllines.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            if re.match(r'^\".*\"$', line):\n",
    "                lines.append(preprocess(line.strip()))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vQQpL8j0fEe"
   },
   "source": [
    "## Text tokenization\n",
    "\n",
    "We start by tokenizing all the text in each line, and then saving the resulting tokenizer. As we will need it at later stages for prediction. We tokenize with the inbuilt [Keras pre-processing tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true,
    "id": "frT5DUIR0GOt",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Use Keras to tokenize the lines.\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def tokenize(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer.texts_to_sequences(lines), tokenizer\n",
    "\n",
    "# Read lines and tokenize them.\n",
    "lines = read_lines()\n",
    "sequences, tokenizer = tokenize(lines)\n",
    "\n",
    "# Save the tokenizer\n",
    "import pickle\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/shakespeare-tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqLF-jVq0hlv"
   },
   "source": [
    "## A function to create n-grams\n",
    "\n",
    "For our model to be able to learn, we need to show it sequences of words (or in our case, integer token representations). We therefore need an n-gram generator for each of our sequences (or lines). This specific n-gram generator produces all possible n-grams from our sequence list (i.e., all lines in the dataset), for each `every_nth` line.\n",
    "\n",
    "```{note}\n",
    "n-grams are contiguous sequences of n items from a given sample of text or speech. In the context of natural language processing (NLP), these items are typically words, though they can also be characters or subword units. The concept of n-grams allows models to capture the context up to n-1 words, providing a foundation for predicting subsequent elements in a sequence. This approach is crucial for understanding the linguistic structure and for tasks such as text prediction, where the likelihood of a word's occurrence may depend significantly on the preceding words.\n",
    "\n",
    "For instance, consider the sentence: \"The quick brown fox jumps over the lazy dog.\" If we were to generate 2-grams (also known as bigrams) for this sentence, we would produce a list of consecutive word pairs: [\"The quick\", \"quick brown\", \"brown fox\", \"fox jumps\", \"jumps over\", \"over the\", \"the lazy\", \"lazy dog\"]. Similarly, 3-grams (trigrams) would yield: [\"The quick brown\", \"quick brown fox\", \"brown fox jumps\", \"fox jumps over\", \"jumps over the\", \"over the lazy\", \"the lazy dog\"]. By examining these n-grams, a model learns the common patterns of word sequences in a language, which aids in predicting what word naturally follows a given sequence.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true,
    "id": "rGCqQP4r0M9Z",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def create_ngrams(sequences, every_nth):\n",
    "    ngrams = []\n",
    "    for token_list in sequences[::every_nth]:\n",
    "        for i in range(1, len(token_list)):\n",
    "            ngram_sequence = token_list[:i+1]\n",
    "            ngrams.append(ngram_sequence)\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFC9FKyh0oY0"
   },
   "source": [
    "## An example n-gram output\n",
    "\n",
    "Here's an example of the generated n-grams for the first five n-grams produced (integers because we are looking at token indices rather than the actual tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "gfi-81dJ0uKA",
    "outputId": "a5846d36-e975-4ae8-92c7-b6891c8557f6",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[307, 3], [194, 39], [194, 39, 6], [194, 39, 6, 112], [194, 39, 6, 112, 16]]\n"
     ]
    }
   ],
   "source": [
    "ngrams = create_ngrams(sequences, 4)\n",
    "print(ngrams[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJLG5Yos06mJ"
   },
   "source": [
    "## Defining the model\n",
    "\n",
    "We now define the model which aims to harness the power of LSTM (Long Short-Term Memory) networks for the purpose of text prediction, particularly focusing on generating text that mimics the style of Shakespearean English. The model's architecture is designed to understand and replicate the intricate patterns of language use found in Shakespeare's works.\n",
    "\n",
    "The model begins with an `Embedding` layer, which is essential for converting the integer-encoded vocabulary into dense vectors of fixed size. This layer acts as a lookup table that allows the model to learn a dense representation of words, capturing semantic meanings that are vital for understanding the context of different words in sentences. The `output_dim` parameter is set to 150, indicating that each word is represented by a 150-dimensional vector. The `input_shape` is determined by `seq_len-1`, preparing the model to process sequences of a specific length, minus one for the word to be predicted.\n",
    "\n",
    "Following the `Embedding` layer, we incorporate an `LSTM` layer with 150 units. LSTM layers are crucial for sequence prediction problems because they can maintain long-term dependencies, overcoming the vanishing gradient problem common in traditional recurrent neural networks (RNNs). The `activation` function used is `tanh`, chosen for its efficacy in LSTMs over alternatives like `relu`, providing a balance between linear and non-linear transformations.\n",
    "\n",
    "The final layer is a `Dense` layer with a `softmax` activation function. This layer outputs a probability distribution over the entire vocabulary for the next word prediction, based on the context provided by the input sequence. The size of this layer matches the vocabulary size, ensuring each word in the vocabulary can be predicted.\n",
    "\n",
    "An `Adam` optimizer with a learning rate of `0.0005` is used to compile the model. This reduced learning rate is a deliberate choice to mitigate the risk of exploding gradients, a common issue in training deep neural networks, especially on complex sequence data. The model uses `sparse_categorical_crossentropy` as the loss function, appropriate for this multi-class classification problem, and tracks `accuracy` as a metric to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true,
    "id": "SDmJ_cRr08wm",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a model with an embeddings layer.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def create_lstm_model(vocab_size, seq_len):\n",
    "    model = Sequential(name=\"LSTM\")\n",
    "    # In Keras3, Embedding doesn't have input_length, instead use input_shape.\n",
    "    model.add(Embedding(vocab_size, output_dim=150, input_shape=(seq_len-1,)))\n",
    "    # Explicitly set the activation function to 'tanh', as 'relu' is not recommended for LSTM layers\n",
    "    model.add(LSTM(150, activation='tanh'))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "    # Adjust learning rate to 1/4 of its default value, 0.001 -> 0.00050 - this is to reduce the chance of exploding gradients\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BvOq6A21Cbz"
   },
   "source": [
    "## Padding sequences\n",
    "\n",
    "Not all sequences (lines of text) are the same size, but the model requires a constant size for its input. This discrepancy poses a challenge in training neural network models, especially those dealing with language processing. To address this issue, a technique known as padding is employed to standardize the length of sequences.\n",
    "\n",
    "Padding involves altering the sequences to ensure they all have the same length. This is achieved by adding a special type of token, often referred to as a \"pad token,\" to the sequences that are shorter than the required length. Conversely, sequences that exceed the desired length can be truncated to fit the model's requirements. The choice of where to pad (beginning or end of the sequence) or truncate (beginning or end) can depend on the specific application and the nature of the data.\n",
    "\n",
    "In practice, padding is applied before feeding the sequences into the model, ensuring that every input sequence has the same shape. This uniformity is crucial for the model to perform batch processing, a computational efficiency technique where multiple sequences are processed simultaneously. Batch processing significantly speeds up the training and inference phases by leveraging parallel computation capabilities.\n",
    "\n",
    "For instance, consider a scenario where we are training a model on a dataset of sentences with varying lengths, and our model requires each input sequence to have a length of 10 words. A sentence with 8 words would be padded with 2 pad tokens at the end (or the beginning, as per the chosen strategy), while a sentence with 12 words might be truncated to the first (or last) 10 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true,
    "id": "oN0EtsNL1Eex",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_seq_len = max([len(x) for x in ngrams])  # Determine the max length after forming n-grams\n",
    "padded_sequences = np.array(pad_sequences(ngrams, maxlen=max_seq_len, padding='pre'))\n",
    "input_sequences = padded_sequences[:, :-1]\n",
    "target_words = padded_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rV9m2uel1KLj"
   },
   "source": [
    "## Verifying sequences\n",
    "\n",
    "Once all sequences have been appropriately padded to meet the model's requirements for uniform input size, it becomes crucial to verify the integrity and appropriateness of these modified sequences. Ensuring the correctness of padded sequences involves checking that there are no empty sequences and that sequences are not solely composed of zeros, which represent the padding tokens. This verification step is fundamental to ensuring the model receives meaningful and accurate data for training.\n",
    "\n",
    "The presence of sequences filled exclusively with pad tokens (zeros) or empty sequences could significantly impact the model's learning process. Such sequences do not provide any valuable information or context that the model can learn from, potentially skewing the training process or leading to biases in the model's predictions. For instance, a model trained on a dataset with a high number of zero-only sequences might become inclined to predict pad tokens more frequently, reducing its overall effectiveness in generating or classifying text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "fHzkmOWK1MqW",
    "outputId": "33cd3c86-690c-45db-da85-992f1aadc58e",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking input sequences...\n",
      "Input sequences are valid.\n",
      "Checking target words...\n",
      "Target words are valid.\n",
      "Max sequence length: 163\n",
      "x size: 175704\n",
      "y size: 175704\n",
      "Vocabulary size: 27390\n",
      "Example input sequence: [[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0  194   39    6  112   16 3956  197]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   194   39    6  112   16 3956  197    4]]\n",
      "Example target word: [   4 9703]\n"
     ]
    }
   ],
   "source": [
    "# Check that the input sequences are not empty, that are not all zeros, and that there are no NaN values. Print an error if they do.\n",
    "def check_input_sequences(input_sequences):\n",
    "    print(\"Checking input sequences...\")\n",
    "    for sequence in input_sequences:\n",
    "        if len(sequence) == 0 or (len(sequence) == 1 and sequence[0] == 0):\n",
    "            print(\"Error: input sequence contains an empty sequence or a sequence of only zeros.\")\n",
    "            return\n",
    "        for i in sequence:\n",
    "            if np.isnan(i) or np.isinf(i):\n",
    "                print(\"Error: input sequence contains NaN or Inf values.\")\n",
    "                return\n",
    "    print(\"Input sequences are valid.\")\n",
    "\n",
    "check_input_sequences(input_sequences)\n",
    "\n",
    "# Check that the target words are not empty, that are not all zeros, and that there are no NaN values. Print an error if they do.\n",
    "def check_target_words(target_words):\n",
    "    print(\"Checking target words...\")\n",
    "    if len(target_words) == 0 or (len(target_words) == 1 and target_words[0] == 0):\n",
    "        print(\"Error: target words contain an empty sequence or a sequence of only zeros.\")\n",
    "        return\n",
    "    for i in target_words:\n",
    "        if np.isnan(i) or np.isinf(i) or i == 0:\n",
    "            print(\"Error: target words contain 0, NaN or Inf values.\")\n",
    "            return\n",
    "    print(\"Target words are valid.\")\n",
    "\n",
    "check_target_words(target_words)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Max sequence length:\", max_seq_len)\n",
    "print(\"x size:\", len(input_sequences))\n",
    "print(\"y size:\", len(target_words))\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"Example input sequence:\", input_sequences[7:9])\n",
    "print(\"Example target word:\", target_words[7:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oc28Ru081OUn"
   },
   "source": [
    "## Early stopping and instantiating the model\n",
    "\n",
    "Overfitting occurs when a model learns the training data too well, capturing noise and outliers in the process, which ultimately leads to poor performance on unseen data. Let's break down the early stopping aspect and then briefly touch on the model creation:\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "**Early stopping** is a form of regularization used to avoid overfitting by halting the training process if the model's performance stops improving on a held-out validation dataset. Here's how the provided `EarlyStopping` callback configuration works:\n",
    "\n",
    "- **`monitor='loss'`**: This specifies that the callback should monitor the training loss. The objective is to minimize this loss. Monitoring allows the callback to track the performance of the model and make decisions based on it.\n",
    "\n",
    "- **`patience=5`**: Patience determines the number of epochs with no improvement after which training will be stopped. In this case, training will continue for 5 more epochs without any improvement in the monitored metric (loss) before stopping.\n",
    "\n",
    "- **`min_delta=0`**: This parameter sets the minimum change in the monitored quantity to qualify as an improvement. Setting it to 0 means any improvement in loss, no matter how small, is considered significant as long as it is a decrease.\n",
    "\n",
    "- **`mode='min'`**: Since the goal is to minimize the loss, the mode is set to 'min'. This means training will stop when the quantity monitored (loss) has stopped decreasing. If the mode were 'max', it would stop training when the monitored quantity stops increasing (useful for metrics like accuracy).\n",
    "\n",
    "- **`verbose=1`**: This enables verbose output, which means the epoch at which training stops will be printed to the console.\n",
    "\n",
    "### LSTM Model Creation\n",
    "\n",
    "Following the early stopping configuration, the code initializes the LSTM model using a `create_lstm_model` function, which is not fully detailed here but likely constructs a sequential model with LSTM layers. The parameters `vocab_size` and `max_seq_len` suggest the model is designed for text processing, where `vocab_size` indicates the size of the vocabulary (number of unique words) the model can recognize, and `max_seq_len` specifies the length of the input sequences.\n",
    "\n",
    "- **`batch_size=32`**: This specifies that 32 samples will be processed before the model's internal parameters are updated. Batch size is an important hyperparameter that can affect the convergence and performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "wuo51Zti1VKi",
    "outputId": "6e4964ce-b871-4db1-a866-6e9d3c29eb3d",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 162, 150)          4108500   \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 150)               180600    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 27390)             4135890   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8424990 (32.14 MB)\n",
      "Trainable params: 8424990 (32.14 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Add early stopping to prevent overfitting.\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='loss',  # Monitor the training loss\n",
    "    patience=5,  # Number of epochs with no improvement after which training will be stopped\n",
    "    min_delta=0,  # Minimum change in the monitored quantity to qualify as an improvement\n",
    "    mode='min',  # In 'min' mode, training will stop when the quantity monitored has stopped decreasing\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Create the LSTM model\n",
    "batch_size = 32\n",
    "model = create_lstm_model(vocab_size, max_seq_len)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NWp-0cZ1aWM"
   },
   "source": [
    "## Training\n",
    "\n",
    "We finally run the training of our LSTM model with the setup previously discussed, including the early stopping mechanism to prevent overfitting. This process involves feeding the model with our preprocessed and padded sequences of text data, along with their corresponding targets, and allowing it to learn how to predict the next word in a sequence based on its current understanding of the text's structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "TwnX_HA71b9V",
    "outputId": "7a3101fd-3aa7-4e4c-a1c6-8bfbe4bcdff7",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "5491/5491 [==============================] - 109s 19ms/step - loss: 6.9633 - accuracy: 0.0431\n",
      "Epoch 2/500\n",
      "5491/5491 [==============================] - 90s 16ms/step - loss: 6.3297 - accuracy: 0.0803\n",
      "Epoch 3/500\n",
      "5491/5491 [==============================] - 88s 16ms/step - loss: 6.0070 - accuracy: 0.0977\n",
      "Epoch 4/500\n",
      "5491/5491 [==============================] - 87s 16ms/step - loss: 5.7569 - accuracy: 0.1085\n",
      "Epoch 5/500\n",
      "5491/5491 [==============================] - 87s 16ms/step - loss: 5.5293 - accuracy: 0.1175\n",
      "Epoch 6/500\n",
      "...\n",
      "...\n",
      "5491/5491 [==============================] - 86s 16ms/step - loss: 1.1493 - accuracy: 0.7512\n",
      "Epoch 135/500\n",
      "5491/5491 [==============================] - 86s 16ms/step - loss: 1.1521 - accuracy: 0.7501\n",
      "Epoch 136/500\n",
      "5491/5491 [==============================] - 87s 16ms/step - loss: 1.1520 - accuracy: 0.7494\n",
      "Epoch 137/500\n",
      "5491/5491 [==============================] - 86s 16ms/step - loss: 1.2544 - accuracy: 0.7220\n",
      "Epoch 138/500\n",
      "5491/5491 [==============================] - 86s 16ms/step - loss: 1.3315 - accuracy: 0.6990\n",
      "Epoch 139/500\n",
      "5491/5491 [==============================] - 87s 16ms/step - loss: 1.1972 - accuracy: 0.7371\n",
      "Epoch 139: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x788ae09b1900>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# Create a unique name for the log directory based on the current time and set callback\n",
    "log_dir = \"logs/fit/\" + model.name + \"_\" + \"batch\" + str(batch_size) + \"-\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(input_sequences,\n",
    "          target_words,\n",
    "          batch_size=batch_size,\n",
    "          epochs=500,\n",
    "          callbacks=[tensorboard_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYsiVHul1ids"
   },
   "source": [
    "## Plotting the training history and saving the model\n",
    "\n",
    "Let us now plot the training history, and save the model. We are saving the trained model on the Google Drive to ensure it doesn't get deleted once the running session ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "editable": true,
    "id": "ctsGc5KL1mHN",
    "outputId": "b26c7354-74bd-433d-be4e-f89a0c77fbf0",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAHHCAYAAACRAnNyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABhu0lEQVR4nO3dd3wUdf7H8dfuJtkUUkhCIIEQAiI1FGkiKiAoIqIgyokoXU8BBfU8Tz0V9QR797Ah6E8BRQGRExFRQZSOdKX3DiEVUnd+f0yysCRAFpJMyvv5eOxjZ2e+O/vZAdm33/nOd2yGYRiIiIiIlEF2qwsQERERORsFFRERESmzFFRERESkzFJQERERkTJLQUVERETKLAUVERERKbMUVERERKTMUlARERGRMktBRURERMosBRWRMsBmszFmzBiv37dz505sNhuTJk0q9prEGoMGDaJOnTrnbdepUyc6depU4vWIWE1BRSTPpEmTsNls2Gw2Fi1aVGC7YRjExsZis9m48cYbLahQRKTyUVAROYO/vz+TJ08usH7BggXs3bsXp9NpQVUiIpWTgorIGW644QamTZtGTk6Ox/rJkyfTqlUratSoYVFllUd6errVJYhIGaGgInKGfv36cezYMebNm+del5WVxVdffcUdd9xR6HvS09N5+OGHiY2Nxel00qBBA1555RXOvDl5ZmYmDz74INWqVSM4OJibbrqJvXv3FrrPffv2MWTIEKpXr47T6aRJkyZ8/PHHF/SdEhMT+cc//kFCQgJVqlQhJCSE7t27s2bNmgJtMzIyGDNmDJdeein+/v5ER0dzyy23sG3bNncbl8vFm2++SUJCAv7+/lSrVo3rr7+eFStWAOceO3PmeJwxY8Zgs9nYuHEjd9xxB1WrVuXKK68EYO3atQwaNIi6devi7+9PjRo1GDJkCMeOHSv0eA0dOpSYmBicTifx8fHcd999ZGVlsX37dmw2G6+//nqB9/3+++/YbDamTJly1uOXlZXFU089RatWrQgNDSUoKIirrrqKn3/+2aNd/vd+5ZVX+OCDD6hXrx5Op5M2bdqwfPnyAvudOXMmTZs2xd/fn6ZNmzJjxoyz1lAUhw8fZujQoVSvXh1/f3+aN2/OJ598UqDd1KlTadWqFcHBwYSEhJCQkMCbb77p3p6dnc0zzzxD/fr18ff3JyIigiuvvNLjvwmR0uJjdQEiZU2dOnVo3749U6ZMoXv37gDMmTOH5ORkbr/9dt566y2P9oZhcNNNN/Hzzz8zdOhQWrRowdy5c3nkkUfYt2+fx4/jsGHD+Oyzz7jjjju44oor+Omnn+jRo0eBGg4dOsTll1+OzWZj5MiRVKtWjTlz5jB06FBSUlIYPXq0V99p+/btzJw5k9tuu434+HgOHTrE+++/T8eOHdm4cSMxMTEA5ObmcuONNzJ//nxuv/12Ro0aRWpqKvPmzWP9+vXUq1cPgKFDhzJp0iS6d+/OsGHDyMnJ4ddff2XJkiW0bt3aq9ry3XbbbdSvX5+xY8e6A968efPYvn07gwcPpkaNGmzYsIEPPviADRs2sGTJEmw2GwD79++nbdu2JCUlcc8999CwYUP27dvHV199xYkTJ6hbty4dOnTg888/58EHH/T43M8//5zg4GBuvvnms9aWkpLCRx99RL9+/bj77rtJTU1lwoQJdOvWjWXLltGiRQuP9pMnTyY1NZW///3v2Gw2XnrpJW655Ra2b9+Or68vAD/88AN9+vShcePGjBs3jmPHjjF48GBq1ap1Qcfv5MmTdOrUia1btzJy5Eji4+OZNm0agwYNIikpiVGjRrmPab9+/ejSpQsvvvgiAH/++Se//fabu82YMWMYN24cw4YNo23btqSkpLBixQpWrVrFtddee0H1iVwwQ0QMwzCMiRMnGoCxfPly45133jGCg4ONEydOGIZhGLfddpvRuXNnwzAMIy4uzujRo4f7fTNnzjQA4z//+Y/H/m699VbDZrMZW7duNQzDMFavXm0AxvDhwz3a3XHHHQZgPP300+51Q4cONaKjo42jR496tL399tuN0NBQd107duwwAGPixInn/G4ZGRlGbm6ux7odO3YYTqfTePbZZ93rPv74YwMwXnvttQL7cLlchmEYxk8//WQAxgMPPHDWNueq68zv+vTTTxuA0a9fvwJt87/n6aZMmWIAxsKFC93rBgwYYNjtdmP58uVnren99983AOPPP/90b8vKyjIiIyONgQMHFnjf6XJycozMzEyPdcePHzeqV69uDBkyxL0u/3tHREQYiYmJ7vXffPONARjffvute12LFi2M6OhoIykpyb3uhx9+MAAjLi7unPUYhmF07NjR6Nixo/v1G2+8YQDGZ5995vH92rdvb1SpUsVISUkxDMMwRo0aZYSEhBg5OTln3Xfz5s09/o6LWEmnfkQK0bdvX06ePMns2bNJTU1l9uzZZz3t89133+FwOHjggQc81j/88MMYhsGcOXPc7YAC7c7sHTEMg6+//pqePXtiGAZHjx51P7p160ZycjKrVq3y6vs4nU7sdvM/99zcXI4dO0aVKlVo0KCBx76+/vprIiMjuf/++wvsI7/34uuvv8Zms/H000+ftc2FuPfeewusCwgIcC9nZGRw9OhRLr/8cgB33S6Xi5kzZ9KzZ89Ce3Pya+rbty/+/v58/vnn7m1z587l6NGj3HnnneeszeFw4Ofn5/68xMREcnJyaN26daF/Fn/729+oWrWq+/VVV10FmD1bAAcOHGD16tUMHDiQ0NBQd7trr72Wxo0bn7OWs/nuu++oUaMG/fr1c6/z9fXlgQceIC0tjQULFgAQFhZGenr6OU/jhIWFsWHDBrZs2XJBtYgUJwUVkUJUq1aNrl27MnnyZKZPn05ubi633nproW137dpFTEwMwcHBHusbNWrk3p7/bLfb3adP8jVo0MDj9ZEjR0hKSuKDDz6gWrVqHo/BgwcD5lgEb7hcLl5//XXq16+P0+kkMjKSatWqsXbtWpKTk93ttm3bRoMGDfDxOftZ4W3bthETE0N4eLhXNZxPfHx8gXWJiYmMGjWK6tWrExAQQLVq1dzt8us+cuQIKSkpNG3a9Jz7DwsLo2fPnh5XdH3++efUrFmTa6655rz1ffLJJzRr1sw9ZqNatWr873//8zh++WrXru3xOj+0HD9+HDj1d6J+/foF3nvm34ei2rVrF/Xr13cH0nxn/j0cPnw4l156Kd27d6dWrVoMGTKE77//3uM9zz77LElJSVx66aUkJCTwyCOPsHbt2guqS+RiaYyKyFnccccd3H333Rw8eJDu3bsTFhZWKp/rcrkAuPPOOxk4cGChbZo1a+bVPseOHcuTTz7JkCFDeO655wgPD8dutzN69Gj35xWns/Ws5ObmnvU9p/ee5Ovbty+///47jzzyCC1atKBKlSq4XC6uv/76C6p7wIABTJs2jd9//52EhARmzZrF8OHDC/y4n+mzzz5j0KBB9OrVi0ceeYSoqCgcDgfjxo3zGGScz+FwFLof44zB1VaIiopi9erVzJ07lzlz5jBnzhwmTpzIgAED3ANvr776arZt28Y333zDDz/8wEcffcTrr7/Oe++9x7Bhwyz+BlLZKKiInEXv3r35+9//zpIlS/jiiy/O2i4uLo4ff/yR1NRUj16Vv/76y709/9nlcrl7LfJt2rTJY3/5VwTl5ubStWvXYvkuX331FZ07d2bChAke65OSkoiMjHS/rlevHkuXLiU7O9s96PNM9erVY+7cuSQmJp61VyW/ByEpKcljff7/1RfF8ePHmT9/Ps888wxPPfWUe/2ZpyOqVatGSEgI69evP+8+r7/+eqpVq8bnn39Ou3btOHHiBHfdddd53/fVV19Rt25dpk+f7hHCCjv9VRT5fycKO7Vy5t8Hb/a5du1aXC6XR/A68+8hgJ+fHz179qRnz564XC6GDx/O+++/z5NPPskll1wCQHh4OIMHD2bw4MGkpaVx9dVXM2bMGAUVKXU69SNyFlWqVGH8+PGMGTOGnj17nrXdDTfcQG5uLu+8847H+tdffx2bzea+cij/+cyrht544w2P1w6Hgz59+vD1118X+uN75MgRr7+Lw+Eo8H/z06ZNY9++fR7r+vTpw9GjRwt8FzjVG9CnTx8Mw+CZZ545a5uQkBAiIyNZuHChx/b//ve/XtV8+j7znXm87HY7vXr14ttvv3VfHl1YTQA+Pj7069ePL7/8kkmTJpGQkFCk3qnCalm6dCmLFy8u8vc5XXR0NC1atOCTTz7xOHU0b948Nm7ceEH7vOGGGzh48KBHqM7JyeHtt9+mSpUqdOzYEaDApd12u919DDIzMwttU6VKFS655BL3dpHSpB4VkXM426mX0/Xs2ZPOnTvzxBNPsHPnTpo3b84PP/zAN998w+jRo91jUlq0aEG/fv3473//S3JyMldccQXz589n69atBfb5wgsv8PPPP9OuXTvuvvtuGjduTGJiIqtWreLHH38kMTHRq+9x44038uyzzzJ48GCuuOIK1q1bx+eff07dunU92g0YMIBPP/2Uhx56iGXLlnHVVVeRnp7Ojz/+yPDhw7n55pvp3Lkzd911F2+99RZbtmxxn4b59ddf6dy5MyNHjgTMS7FfeOEFhg0bRuvWrVm4cCGbN28ucs0hISFcffXVvPTSS2RnZ1OzZk1++OEHduzYUaDt2LFj+eGHH+jYsSP33HMPjRo14sCBA0ybNo1FixZ5nLYbMGAAb731Fj///LP78tyiHL/p06fTu3dvevTowY4dO3jvvfdo3LgxaWlpRf5Opxs3bhw9evTgyiuvZMiQISQmJvL222/TpEmTC9rnPffcw/vvv8+gQYNYuXIlderU4auvvuK3337jjTfecPf2DRs2jMTERK655hpq1arFrl27ePvtt2nRooV7PEvjxo3p1KkTrVq1Ijw8nBUrVvDVV1+5/2xFSpVFVxuJlDmnX558LmdenmwYhpGammo8+OCDRkxMjOHr62vUr1/fePnll92XxuY7efKk8cADDxgRERFGUFCQ0bNnT2PPnj0FLtk1DMM4dOiQMWLECCM2Ntbw9fU1atSoYXTp0sX44IMP3G28uTz54YcfNqKjo42AgACjQ4cOxuLFiwtc4moY5iXBTzzxhBEfH+/+3FtvvdXYtm2bu01OTo7x8ssvGw0bNjT8/PyMatWqGd27dzdWrlzpsZ+hQ4caoaGhRnBwsNG3b1/j8OHDZ708+ciRIwXq3rt3r9G7d28jLCzMCA0NNW677TZj//79hR6vXbt2GQMGDDCqVatmOJ1Oo27dusaIESMKXFZsGIbRpEkTw263G3v37j3nccvncrmMsWPHGnFxcYbT6TRatmxpzJ492xg4cKDHpcT5fx4vv/xygX0UVvPXX39tNGrUyHA6nUbjxo2N6dOnF9jn2RT2Z3fo0CFj8ODBRmRkpOHn52ckJCQU+Lvx1VdfGdddd50RFRVl+Pn5GbVr1zb+/ve/GwcOHHC3+c9//mO0bdvWCAsLMwICAoyGDRsazz//vJGVlXXeukSKm80wysDoLhGRUtSyZUvCw8OZP3++1aWIyHlojIqIVCorVqxg9erVDBgwwOpSRKQI1KMiIpXC+vXrWblyJa+++ipHjx5l+/bt+Pv7W12WiJyHelREpFL46quvGDx4MNnZ2UyZMkUhRaScUI+KiIiIlFnqUREREZEyS0FFREREyqxyPeGby+Vi//79BAcHX9RdW0VERKT0GIZBamoqMTEx573XVrkOKvv37yc2NtbqMkREROQC7Nmzh1q1ap2zTbkOKvlTQu/Zs4eQkBCLqxEREZGiSElJITY21uNGrmdTroNK/umekJAQBRUREZFypijDNjSYVkRERMosBRUREREpsxRUREREpMwq12NUiio3N5fs7GyryxAv+fr64nA4rC5DREQsVKGDimEYHDx4kKSkJKtLkQsUFhZGjRo1NE+OiEglVaGDSn5IiYqKIjAwUD925YhhGJw4cYLDhw8DEB0dbXFFIiJihQobVHJzc90hJSIiwupy5AIEBAQAcPjwYaKionQaSESkEqqwg2nzx6QEBgZaXIlcjPw/P40xEhGpnCwNKnXq1MFmsxV4jBgxotg+Q6d7yjf9+YmIVG6WnvpZvnw5ubm57tfr16/n2muv5bbbbrOwKhERESkrLA0q1apV83j9wgsvUK9ePTp27GhRRSIiIlKWlJkxKllZWXz22WcMGTLkrN39mZmZpKSkeDwqssWLF+NwOOjRo4fVpYiIiFiizASVmTNnkpSUxKBBg87aZty4cYSGhrofsbGxJVOMYUBOFuRklsz+i2jChAncf//9LFy4kP3791tWR1ZWlmWfLSIilVuZCSoTJkyge/fuxMTEnLXNY489RnJysvuxZ8+ekinmxDE4vAGS95bM/osgLS2NL774gvvuu48ePXowadIkj+3ffvstbdq0wd/fn8jISHr37u3elpmZyaOPPkpsbCxOp5NLLrmECRMmADBp0iTCwsI89jVz5kyPXqwxY8bQokULPvroI+Lj4/H39wfg+++/58orryQsLIyIiAhuvPFGtm3b5rGvvXv30q9fP8LDwwkKCqJ169YsXbqUnTt3YrfbWbFihUf7N954g7i4OFwu18UeMhERqYDKxDwqu3bt4scff2T69OnnbOd0OnE6nRf8OYZhcDI79/wNc+2Q7QIjE7JyLvjz8gX4Ory+euXLL7+kYcOGNGjQgDvvvJPRo0fz2GOPYbPZ+N///kfv3r154okn+PTTT8nKyuK7775zv3fAgAEsXryYt956i+bNm7Njxw6OHj3q1edv3bqVr7/+munTp7vnL0lPT+ehhx6iWbNmpKWl8dRTT9G7d29Wr16N3W4nLS2Njh07UrNmTWbNmkWNGjVYtWoVLpeLOnXq0LVrVyZOnEjr1q3dnzNx4kQGDRqE3V5mMrOIiJQhZSKoTJw4kaioqBIfi3EyO5fGT8318l07LvpzNz7bjUA/7w71hAkTuPPOOwG4/vrrSU5OZsGCBXTq1Innn3+e22+/nWeeecbdvnnz5gBs3ryZL7/8knnz5tG1a1cA6tat63XNWVlZfPrppx4Dnvv06ePR5uOPP6ZatWps3LiRpk2bMnnyZI4cOcLy5csJDw8H4JJLLnG3HzZsGPfeey+vvfYaTqeTVatWsW7dOr755huv6xMRkcrB8v+NdblcTJw4kYEDB+LjUyZyk+U2bdrEsmXL6NevHwA+Pj787W9/c5++Wb16NV26dCn0vatXr8bhcFz0lVNxcXEFrsrasmUL/fr1o27duoSEhFCnTh0Adu/e7f7sli1bukPKmXr16oXD4WDGjBmAeRqqc+fO7v2IiIicyfJk8OOPP7J7926GDBlS4p8V4Otg47Pdzt/QMODgOsCAao3Ax++iP9cbEyZMICcnx2O8jmEYOJ1O3nnnHffU8oV+1jm2AdjtdgzD8FhX2KyvQUFBBdb17NmTuLg4PvzwQ2JiYnC5XDRt2tQ92PZ8n+3n58eAAQOYOHEit9xyC5MnT+bNN98853tERKRyszyoXHfddQV+OEuKzWYr+ikYfyfkZoHDBV6etrkYOTk5fPrpp7z66qtcd911Htt69erFlClTaNasGfPnz2fw4MEF3p+QkIDL5WLBggXuUz+nq1atGqmpqaSnp7vDyOrVq89b17Fjx9i0aRMffvghV111FQCLFi3yaNOsWTM++ugjEhMTz9qrMmzYMJo2bcp///tfcnJyuOWWW8772SIiUnlZHlTKLIevGVRcpXuPmdmzZ3P8+HGGDh1KaGiox7Y+ffowYcIEXn75Zbp06UK9evW4/fbbycnJ4bvvvuPRRx+lTp06DBw4kCFDhrgH0+7atYvDhw/Tt29f2rVrR2BgII8//jgPPPAAS5cuLXBFUWGqVq1KREQEH3zwAdHR0ezevZt//etfHm369evH2LFj6dWrF+PGjSM6Opo//viDmJgY2rdvD0CjRo24/PLLefTRRxkyZMh5e2FERKRys3yMSpll9zWfc0s3qEyYMIGuXbsWCClgBpUVK1YQHh7OtGnTmDVrFi1atOCaa65h2bJl7nbjx4/n1ltvZfjw4TRs2JC7776b9PR0AMLDw/nss8/47rvvSEhIYMqUKYwZM+a8ddntdqZOncrKlStp2rQpDz74IC+//LJHGz8/P3744QeioqK44YYbSEhI4IUXXihw1+OhQ4eSlZVVKqf7RESkfLMZpXXepQSkpKQQGhpKcnIyISEhHtsyMjLYsWOHxzwgXkneB+mHISgKQmsWU8UC8NxzzzFt2jTWrl173rYX/ecoIiJlzrl+v8+kHpWzceT3qGhW1uKSlpbG+vXreeedd7j//vutLkdERMoBBZWzyQ8qpTxGpSIbOXIkrVq1olOnTjrtIyIiRaLBtGdj0RiVimzSpElFGrgrIiKSTz0qZ+M4LaiU32E8IiIi5ZqCytnkBxUMcF38/X5ERETEewoqZ2Ozgz3vzJhO/4iIiFhCQeVc7BpQKyIiYiUFlXNxaECtiIiIlRRUzkVBRURExFIKKufiyLtrskuTvomIiFhBQeVcLOpRGTRoEL169SrVzxQRESmLFFTORZO+iYiIWEpB5VzK4BiVBQsW0LZtW5xOJ9HR0fzrX/8iJ+fUPC9fffUVCQkJBAQEEBERQdeuXd13Tv7ll19o27YtQUFBhIWF0aFDB3bt2mXVVxERETmvyjWFvmFA9omit3flQPZJczkjBeyOC/tc30Cw2S7svafZt28fN9xwA4MGDeLTTz/lr7/+4u6778bf358xY8Zw4MAB+vXrx0svvUTv3r1JTU3l119/xTAMcnJy6NWrF3fffTdTpkwhKyuLZcuWYSuGukREREpK5Qoq2SdgbEzpf+7j+8Ev6KJ389///pfY2FjeeecdbDYbDRs2ZP/+/Tz66KM89dRTHDhwgJycHG655Rbi4uIASEhIACAxMZHk5GRuvPFG6tWrB0CjRo0uuiYREZGSpFM/5ciff/5J+/btPXpBOnToQFpaGnv37qV58+Z06dKFhIQEbrvtNj788EOOHz8OQHh4OIMGDaJbt2707NmTN998kwMHDlj1VURERIqkcvWo+AaavRveOLYNstIgtDYEVr3wzy0FDoeDefPm8fvvv/PDDz/w9ttv88QTT7B06VLi4+OZOHEiDzzwAN9//z1ffPEF//73v5k3bx6XX355qdQnIiLircrVo2KzmadgvHn4h4BvgDmw1tv35j+KaRxIo0aNWLx4McZpd3P+7bffCA4OplatWnlf0UaHDh145pln+OOPP/Dz82PGjBnu9i1btuSxxx7j999/p2nTpkyePLlYahMRESkJlatH5UI4rLnfT3JyMqtXr/ZYd8899/DGG29w//33M3LkSDZt2sTTTz/NQw89hN1uZ+nSpcyfP5/rrruOqKgoli5dypEjR2jUqBE7duzggw8+4KabbiImJoZNmzaxZcsWBgwYUKrfS0RExBsKKudjz5udNrd0Z6f95ZdfaNmypce6oUOH8t133/HII4/QvHlzwsPDGTp0KP/+978BCAkJYeHChbzxxhukpKQQFxfHq6++Svfu3Tl06BB//fUXn3zyCceOHSM6OpoRI0bw97//vVS/l4iIiDdsxunnEcqZlJQUQkNDSU5OJiQkxGNbRkYGO3bsID4+Hn9//wv/kJNJcHyHOc6kWoOLK1i8Vmx/jiIiUmac6/f7TJVrjMqFKIOTvomIiFQWCirnc/oYFcNlbS0iIiKVjILK+dh9wZY3I212hrW1iIiIVDIKKudjs4Ff3jwo2enW1iIiIlLJVPigUixjhfMnbMvy4j5BUizK8VhvEREpBhU2qPj6mmNLTpwohnCRH1S8uaGhFIv8P7/8P08REalcKuw8Kg6Hg7CwMA4fPgxAYGDghd8p2OWAHANyTsKJ9Au/i7IUmWEYnDhxgsOHDxMWFobDoWMuIlIZVdigAlCjRg0Ad1i5KCnHwZUDyXbwcV78/qRIwsLC3H+OIiJS+VTooGKz2YiOjiYqKors7IucB+V/78KOX6DDaGh5Z3GUJ+fh6+urnhQRkUquQgeVfA6H4+J/8KLiYd3/wd5F0H5Y8RQmIiIi51RhB9MWu5jLzOd9q6ytQ0REpBJRUCmqmLwbBCbtgvRj1tYiIiJSSSioFFVAGERcYi7vV6+KiIhIabA8qOzbt48777yTiIgIAgICSEhIYMWKFVaXVbiarcxnnf4REREpFZYGlePHj9OhQwd8fX2ZM2cOGzdu5NVXX6Vq1apWlnV27nEqK62tQ0REpJKw9KqfF198kdjYWCZOnOheFx8fb2FF51EzL6jsXwWGYd4HSEREREqMpT0qs2bNonXr1tx2221ERUXRsmVLPvzwQytLOrcaCWD3gfQjkLzX6mpEREQqPEuDyvbt2xk/fjz169dn7ty53HfffTzwwAN88sknhbbPzMwkJSXF41GqfAOgehNzefeS0v1sERGRSsjSoOJyubjssssYO3YsLVu25J577uHuu+/mvffeK7T9uHHjCA0NdT9iY2NLuWIg/mrzefvPpf/ZIiIilYylQSU6OprGjRt7rGvUqBG7d+8utP1jjz1GcnKy+7Fnz57SKNNT3c7m87afzXEqIiIiUmIsHUzboUMHNm3a5LFu8+bNxMXFFdre6XTidFp8Q8C4K8DhhNT9cHQzVGtgbT0iIiIVmKU9Kg8++CBLlixh7NixbN26lcmTJ/PBBx8wYsQIK8s6N98AqH25ubz9F0tLERERqegsDSpt2rRhxowZTJkyhaZNm/Lcc8/xxhtv0L9/fyvLOr96p53+ERERkRJjM4zyO9AiJSWF0NBQkpOTCQkJKb0P3r8aPugIfsHw6A5w+JbeZ4uIiJRz3vx+Wz6FfrlUoxkERkBWKuwto9P9i4iIVAAKKhfCbof4juayLlMWEREpMQoqF0rjVEREREqcgsqFyp9PZd9KyEi2thYREZEKSkHlQoXFQsQlYOTCjl+trkZERKRCUlC5GO5ZaudbW4eIiEgFpaByMepfZz5vnqvp9EVEREqAgsrFiL8afAMhZR8cXGt1NSIiIhWOgsrF8PU/dfpn0/fW1iIiIlIBKahcrAbXm8+b51hbh4iISAWkoHKx6nczn/f/ASkHrK1FRESkglFQuVjB1aFmK3N5y1xraxEREalgFFSKw6XdzWeNUxERESlWCirFIX+cyvZfIPukpaWIiIhUJAoqxaF6UwipBTknYfsCq6sRERGpMBRUioPNpqt/RERESoCCSnHJH6eiWWpFRESKjYJKcalzJfgGQeoBOLDa6mpEREQqBAWV4uLrD/U0S62IiEhxUlApTg3yT/9onIqIiEhxUFApTvW7ATY4sAaS91ldjYiISLmnoFKcqlSDWq3N5c06/SMiInKxFFSK26X5lykrqIiIiFwsBZXilj9OZfsCyEq3thYREZFyTkGluEU1htDakJtpTqkvIiIiF0xBpbidPkvtJl39IyIicjEUVEqCe5zKXHC5rK1FRESkHFNQKQl1rgS/YEg/DPtWWl2NiIhIuaWgUhJ8nHDpdebyn7OsrUVERKQcU1ApKY16ms9/fqubFIqIiFwgBZWScsm14HDC8R1waIPV1YiIiJRLCiolxVkFLuliLv/5rbW1iIiIlFMKKiXp9NM/IiIi4jUFlZJ06fVgc8DhDXBsm9XViIiIlDsKKiUpMBzirzKX1asiIiLiNQWVkqbTPyIiIhdMQaWkNbwRsMG+FZC8z+pqREREyhUFlZIWXANi25rLmvxNRETEK5YGlTFjxmCz2TweDRs2tLKkktGkt/m8frq1dYiIiJQzlveoNGnShAMHDrgfixYtsrqk4te4F2CDvcsgaY/V1YiIiJQblgcVHx8fatSo4X5ERkZaXVLxC4mGuCvM5Q0zrK1FRESkHLE8qGzZsoWYmBjq1q1L//792b1791nbZmZmkpKS4vEoN5reYj5v0OkfERGRorI0qLRr145Jkybx/fffM378eHbs2MFVV11Fampqoe3HjRtHaGio+xEbG1vKFV+ERjeDzQ77/9DkbyIiIkVkM4yyc2vfpKQk4uLieO211xg6dGiB7ZmZmWRmZrpfp6SkEBsbS3JyMiEhIaVZ6oX59GbY/gtc8yRc/Q+rqxEREbFESkoKoaGhRfr9tvzUz+nCwsK49NJL2bp1a6HbnU4nISEhHo9ypUn+6R+NUxERESmKMhVU0tLS2LZtG9HR0VaXUjIa9QS7DxxaD0c2WV2NiIhImWdpUPnHP/7BggUL2LlzJ7///ju9e/fG4XDQr18/K8sqOYHhUO8ac1lzqoiIiJyXpUFl79699OvXjwYNGtC3b18iIiJYsmQJ1apVs7KsktXktKt/ys7wIBERkTLJx8oPnzp1qpUfb42GN4DDCUc3w6ENUKOp1RWJiIiUWWVqjEql4B8K9a81l9d/bW0tIiIiZZyCihXy7/2j0z8iIiLnpKBihUuvB58AOL7TnABOREREClUsQSUpKak4dlN5OKtAg+vNZZ3+EREROSuvg8qLL77IF1984X6df7VOzZo1WbNmTbEWV6G5r/6ZCS6XpaWIiIiUVV4Hlffee899j5158+Yxb9485syZQ/fu3XnkkUeKvcAKq/614FcFUvbC3uVWVyMiIlImeX158sGDB91BZfbs2fTt25frrruOOnXq0K5du2IvsMLyDYCGPWDtF+ag2to6diIiImfyukelatWq7NmzB4Dvv/+erl27AmAYBrm5ucVbXUWXf/pn/XTIzbG2FhERkTLI66Byyy23cMcdd3Dttddy7NgxunfvDsAff/zBJZdcUuwFVmj1roGAcEg/DNt/troaERGRMsfroPL6668zcuRIGjduzLx586hSpQoABw4cYPjw4cVeYIXm4wcJt5nLa6ZYW4uIiEgZZDOM8jvjWEpKCqGhoSQnJxMSEmJ1ORdm3yr4sDP4+MM/Npsz14qIiFRg3vx+e92j8sknn/C///3P/fqf//wnYWFhXHHFFezatcv7aiu7mJYQ2QByMsxLlUVERMTN66AyduxYAgICAFi8eDHvvvsuL730EpGRkTz44IPFXmCFZ7NBi37m8ppKeJNGERGRc/A6qOzZs8c9aHbmzJn06dOHe+65h3HjxvHrr78We4GVQkJfwAa7f4fEHVZXIyIiUmZ4HVSqVKnCsWPHAPjhhx+49lrzTsD+/v6cPHmyeKurLEJrQt2O5vLaL87dVkREpBLxOqhce+21DBs2jGHDhrF582ZuuOEGADZs2ECdOnWKu77Ko/kd5vOaKbqjsoiISB6vg8q7775L+/btOXLkCF9//TUREREArFy5kn79+hV7gZVGoxvNKfWP74Rdv1tdjYiISJmgy5PLkm9GwB+fQYv+0Ou/VlcjIiJSIrz5/fb6Xj8ASUlJTJgwgT///BOAJk2aMGTIEEJDNQfIRWl5lxlUNsyA7i+CM9jqikRERCzl9amfFStWUK9ePV5//XUSExNJTEzktddeo169eqxataokaqw8YttBxCWQfcIMKyIiIpWc10HlwQcf5KabbmLnzp1Mnz6d6dOns2PHDm688UZGjx5dAiVWIjYbtLzTXP7jM2trERERKQMuqEfl0Ucfxcfn1FkjHx8f/vnPf7JixYpiLa5Sat4PbA7YsxSObLa6GhEREUt5HVRCQkLYvXt3gfV79uwhOFhjKi5acA2ob85Nw2r1qoiISOXmdVD529/+xtChQ/niiy/Ys2cPe/bsYerUqQwbNkyXJxeX/NM/a6ZCbo61tYiIiFjI66t+XnnlFWw2GwMGDCAnx/wR9fX15b777uOFF14o9gIrpfrdIDAS0g7B1nnQoLvVFYmIiFjigudROXHiBNu2bQOgXr16+Pn5cfjwYWJiYoq1wHOpcPOonG7uE7D4Hbi0O9yhmxWKiEjF4c3vt9enfvIFBgaSkJBAQkICgYGBbNiwgdjY2AvdnZyp1SDzectcSN5naSkiIiJWueCgIiUssj7EXQmGS5cqi4hIpaWgUpbl96qs+hRcuZaWIiIiYgUFlbKsUU8IqAope2HrfKurERERKXVFvupn7dq159y+adOmiy5GzuDrb04At+S/sHISXHqd1RWJiIiUqiIHlRYtWmCz2SjsIqH89TabrViLE+CygWZQ2fw9pOyHkNK7qkpERMRqRQ4qO3bsKMk65GyiGkLt9rB7sTmotuM/ra5IRESk1BQ5qMTFxZVkHXIurQabQWXlJLjyIXB4PU+fiIhIuaTBtOVB45shMAJS9sHmOVZXIyIiUmoUVMoDX3+4bIC5vOwDa2sREREpRQoq5UXrIWCzw46FcERXWImISOWgoFJehNU27/sDsPwja2sREREpJWUmqLzwwgvYbDZGjx5tdSllV9th5vPqKZCZam0tIiIipaBIl4+0bNmyyHOkrFq1yusili9fzvvvv0+zZs28fm+lEt8JIurDsS2wZiq0vdvqikREREpUkXpUevXqxc0338zNN99Mt27d2LZtG06nk06dOtGpUyf8/f3Ztm0b3bp187qAtLQ0+vfvz4cffkjVqlW9fn+lYrdDm7xeleUfQSGT74mIiFQkRepRefrpp93Lw4YN44EHHuC5554r0GbPnj1eFzBixAh69OhB165d+c9//nPOtpmZmWRmZrpfp6SkeP155V6LfvDTc3DkL9j2E1zSxeqKRERESozXY1SmTZvGgAEDCqy/8847+frrr73a19SpU1m1ahXjxo0rUvtx48YRGhrqfsTGxnr1eRWCfyi0vNNcXvJfa2sREREpYV4HlYCAAH777bcC63/77Tf8/f2LvJ89e/YwatQoPv/88yK/77HHHiM5Odn9uJAenAqh3d8BG2z9UZcqi4hIheb1XOyjR4/mvvvuY9WqVbRt2xaApUuX8vHHH/Pkk08WeT8rV67k8OHDXHbZZe51ubm5LFy4kHfeeYfMzEwcDofHe5xOJ06n09uSK57wutCwB/w12+xV6fmm1RWJiIiUCJtR2O2Qz+PLL7/kzTff5M8//wSgUaNGjBo1ir59+xZ5H6mpqezatctj3eDBg2nYsCGPPvooTZs2Pe8+UlJSCA0NJTk5mZCQEO++RHm38zeYdAP4+MODGyEowuqKREREisSb3+8Lurtd3759vQolhQkODi4QRoKCgoiIiChSSKn04q6A6OZwYA2s/BiufsTqikRERIrdBU/4lpWVxd69e9m9e7fHQ0qJzQaXjzCXl30EOVnW1iMiIlICvO5R2bJlC0OGDOH333/3WG8YBjabjdzc3Asu5pdffrng91ZKTXrDj09D6gFY9+Wpq4FEREQqCK+DyqBBg/Dx8WH27NlER0cXecZaKQE+fnD5fTDvKVj0BjS/w5wUTkREpILwOqisXr2alStX0rBhw5KoR7zVajAsfNWcVn/T/6BRT6srEhERKTZe/+9348aNOXr0aEnUIhfCP+TUzQoXva5p9UVEpELxOqi8+OKL/POf/+SXX37h2LFjpKSkeDzEAu3uNS9T3rcSdi6yuhoREZFi4/U8Kva8MRBnjk0pjsG03qrU86ic6X8PmzcqrNcF7ppudTUiIiJnVaLzqPz8888XXJiUoCvuhxUTYdt82L8aYlpYXZGIiMhF8zqodOzYsSTqkItVtQ40vQXWTYNfX4W//Z/VFYmIiFy0C5qZFuDEiRPs3r2brCzPicaaNWt20UXJBbrqYTOo/DkLDm2A6k2srkhEROSieB1Ujhw5wuDBg5kzZ06h20tzjIqcIaoRNL4ZNn4DC1+G2yZZXZGIiMhF8fqqn9GjR5OUlMTSpUsJCAjg+++/55NPPqF+/frMmjWrJGoUb1z9T/N5w0w4/JelpYiIiFwsr4PKTz/9xGuvvUbr1q2x2+3ExcVx55138tJLLzFu3LiSqFG8UaMpNLwRMMxeFRERkXLM66CSnp5OVFQUAFWrVuXIkSMAJCQksGrVquKtTi5Mx0fN5/Vfw5HN1tYiIiJyEbwOKg0aNGDTpk0ANG/enPfff599+/bx3nvvER0dXewFygWIbgYNbkC9KiIiUt55HVRGjRrFgQMHAHj66aeZM2cOtWvX5q233mLs2LHFXqBcoPxelXXTzCuAREREyiGvZ6Y904kTJ/jrr7+oXbs2kZGRxVVXkWhm2vP4coB5BVCDG6DfFKurERERAbz7/fa6R+VMgYGBXHbZZaUeUqQIOv8bbHbY9B3sXmp1NSIiIl676KAiZVi1S6FFf3N5/rO6s7KIiJQ7CioVXad/gcMJuxaZ9wESEREpRxRUKrrQWtBmmLn84zPgcllbj4iIiBcUVCqDqx4Gv2A4uBbWf2V1NSIiIkXmdVCpU6cOzz77LLt37y6JeqQkBEXAVQ+ay/OfhewMa+sREREpogu618/06dOpW7cu1157LVOnTiUzM7MkapPidPlwCKkJyXtg6XtWVyMiIlIkFxRUVq9ezbJly2jUqBH3338/0dHRjBw5UlPol2W+AXDNk+byr69C+jFr6xERESmCCx6jctlll/HWW2+xf/9+nn76aT766CPatGlDixYt+Pjjj7nIeeSkJDT7G9RIgMwUWPCi1dWIiIic1wUHlezsbL788ktuuukmHn74YVq3bs1HH31Enz59ePzxx+nfv39x1inFwW6H6543l1dMgKNbrK1HRETkPHy8fcOqVauYOHEiU6ZMwW63M2DAAF5//XUaNmzobtO7d2/atGlTrIVKManbES69HjZ/D3MehTu/BpvN6qpEREQK5XWPSps2bdiyZQvjx49n3759vPLKKx4hBSA+Pp7bb7+92IqUYtZtLDj8zAng/pptdTUiIiJn5fVNCXft2kVcXFxJ1eMV3ZTwIsx/Dn59BUJjYcQy8Au0uiIREakkSvSmhIcPH2bp0oI3uFu6dCkrVqzwdndilasegpBa5uXKi163uhoREZFCeR1URowYwZ49ewqs37dvHyNGjCiWoqQU+AXB9WPN5d/ehGPbrK1HRESkEF4HlY0bN3LZZZcVWN+yZUs2btxYLEVJKWl0E9TtBLmZMOefuruyiIiUOV4HFafTyaFDhwqsP3DgAD4+Xl9EJFay2eCGV8yBtVt/hA0zrK5IRETEg9dB5brrruOxxx4jOTnZvS4pKYnHH3+ca6+9tliLk1IQWR+ufMhc/v5fkJF87vYiIiKlyOug8sorr7Bnzx7i4uLo3LkznTt3Jj4+noMHD/Lqq6+WRI1S0q58EMLrQdoh82ogERGRMsLry5MB0tPT+fzzz1mzZg0BAQE0a9aMfv364evrWxI1npUuTy5G2xfApzcBNhg2H2q1sroiERGpoLz5/b6goFJWKKgUs+l/h7VToXpTuPtn8PGzuiIREamAvPn9vuDRrxs3bmT37t1kZWV5rL/pppsudJditW7Pw9Z5cGi9ORlc58etrkhERCo5r4PK9u3b6d27N+vWrcNms7nvkmzLu19Mbm5ukfc1fvx4xo8fz86dOwFo0qQJTz31FN27d/e2LCkOQZHmVUBfDYZfX4WGPSC6udVViYhIJeb1YNpRo0YRHx/P4cOHCQwMZMOGDSxcuJDWrVvzyy+/eLWvWrVq8cILL7By5UpWrFjBNddcw80338yGDRu8LUuKS9NboPHN4MqBGfdBTtb53yMiIlJCvB6jEhkZyU8//USzZs0IDQ1l2bJlNGjQgJ9++omHH36YP/7446IKCg8P5+WXX2bo0KHnbasxKiUk7Qj8tx2cOAZXPwLX/NvqikREpAIp0Xv95ObmEhwcDJihZf/+/QDExcWxadOmCyj31H6nTp1Keno67du3L7RNZmYmKSkpHg8pAVWqQY+8S81/fQ32LLe2HhERqbS8DipNmzZlzZo1ALRr146XXnqJ3377jWeffZa6det6XcC6deuoUqUKTqeTe++9lxkzZtC4ceNC244bN47Q0FD3IzY21uvPkyJq0hua3gpGLkwfBpmpVlckIiKVkNenfubOnUt6ejq33HILW7du5cYbb2Tz5s1ERETwxRdfcM0113hVQFZWFrt37yY5OZmvvvqKjz76iAULFhQaVjIzM8nMzHS/TklJITY2Vqd+SsrJJHjvKkjeDc3vgN7jra5IREQqgFKfRyUxMZGqVau6r/y5GF27dqVevXq8//77522rMSqlYNfvMKkHGC649WNo2sfqikREpJwrsTEq2dnZ+Pj4sH79eo/14eHhxRJSAFwul0eviVgs7gq46mFz+dsHIWm3tfWIiEil4lVQ8fX1pXbt2l7NlXIujz32GAsXLmTnzp2sW7eOxx57jF9++YX+/fsXy/6lmHR8FGq2hsxkmDZYlyyLiEip8Xow7RNPPMHjjz9OYmLiRX/44cOHGTBgAA0aNKBLly4sX76cuXPn6i7MZY3DF26dAP6hsG8FzH/G6opERKSS8HqMSsuWLdm6dSvZ2dnExcURFBTksX3VqlXFWuC5aIxKKftzNnyR19t1+xRoeIO19YiISLlUovf66dWr14XWJeVdoxvh8uGw5L8w8z6491cIq211VSIiUoHp7sninZwsmHg97FsJ0S1gyPfgG2B1VSIiUo6U6My0Usn5+MFtkyAwAg6shlkPQPnNuiIiUsZ5HVTsdjsOh+OsD6kEwmrDbZ+AzQHrvoTF71hdkYiIVFBej1GZMWOGx+vs7Gz++OMPPvnkE555RleDVBrxV8H1L8CcR2DeUxDVGC7pYnVVIiJSwRTbGJXJkyfzxRdf8M033xTH7opEY1QsZhgwayT88Rk4Q2HoDxDV0OqqRESkjLNkjMrll1/O/Pnzi2t3Uh7YbNDjNYi93JwMbvJtkHbY6qpERKQCKZagcvLkSd566y1q1qxZHLuT8sTHCbdPhvC65vT6k/8GWSesrkpERCoIr8eonHnzQcMwSE1NJTAwkM8++6xYi5NyIigC+n8FH3WB/atg+t3Q91Owa3C1iIhcHK+Dyuuvv+4RVOx2O9WqVaNdu3ZUrVq1WIuTciSinjlb7ac3wV+z4bt/mKeFiulmlSIiUjlpwjcpXhtmmDcuxDBvZtj5casrEhGRMqZEB9NOnDiRadOmFVg/bdo0PvnkE293JxVNk97Q4xVzecGLsPR9a+sREZFyzeugMm7cOCIjIwusj4qKYuzYscVSlJRzbYZBp7yelDn/hNVTrK1HRETKLa+Dyu7du4mPjy+wPi4ujt27dxdLUVIBdPwntP27uTzzPlg92dp6RESkXPI6qERFRbF27doC69esWUNERESxFCUVgM0G3V+E1kMBA2YONyeGExER8YLXQaVfv3488MAD/Pzzz+Tm5pKbm8tPP/3EqFGjuP3220uiRimvbDbo8ap5KggDvhkJq/7P6qpERKQc8fry5Oeee46dO3fSpUsXfHzMt7tcLgYMGKAxKlKQzQY3vAI2Oyz7AGbdb66/7C5r6xIRkXLhgi9P3rJlC6tXryYgIICEhATi4uKKu7bz0uXJ5YhhwJxHYVneVUA934JWA62tSURELOHN77fXPSr56tevT/369S/07VLZ5I9Zsdlg6Xvw7QNguKD1YKsrExGRMszrMSp9+vThxRdfLLD+pZde4rbbbiuWoqSCstng+heg3X3m69mjYdHrZm+LiIhIIbwOKgsXLuSGG24osL579+4sXLiwWIqSCsxmg+vHQYfR5usfx8DcJ8DlsrIqEREpo7wOKmlpafj5+RVY7+vrS0pKSrEUJRWczQbXPgPXPW++XvIuzLgHcjKtrUtERMocr4NKQkICX3zxRYH1U6dOpXHjxsVSlFQSV4yE3h+A3QfWTYNPboK0I1ZXJSIiZYjXg2mffPJJbrnlFrZt28Y111wDwPz585kyZUqh9wASOafmf4OgSPNGhnuWwIfXQL8pUKOp1ZWJiEgZ4HWPSs+ePZk5cyZbt25l+PDhPPzww+zdu5cff/yRXr16lUCJUuFd0gWG/QjhdSF5N0y4Dv76n9VViYhIGXDB86gUZv369TRtWnr/J6x5VCqYE4kwbRDsWADYoMuTcOVD5pgWERGpMLz5/fa6R+VMqampfPDBB7Rt25bmzZtf7O6kMgsMhzu/hjZ3AwbMfxam3wPZGVZXJiIiFrngoLJw4UIGDBhAdHQ0r7zyCtdccw1LliwpztqkMnL4Qo9XzHsE2Ryw7kuYcC0c22Z1ZSIiYgGvBtMePHiQSZMmMWHCBFJSUujbty+ZmZnMnDlTV/xI8WozDCLqm6eCDq6F96+GHq+Zg29FRKTSKHKPSs+ePWnQoAFr167ljTfeYP/+/bz99tslWZtUdnU7wn2/QdyVkJVmzrUy417ITLO6MhERKSVFDipz5sxh6NChPPPMM/To0QOHw1GSdYmYQmJg4Czo9Lh5B+Y1U+CDjnBgjdWViYhIKShyUFm0aBGpqam0atWKdu3a8c4773D06NGSrE3EZHdAp0dh4GwIqQnHtsJHXWHJe7pPkIhIBVfkoHL55Zfz4YcfcuDAAf7+978zdepUYmJicLlczJs3j9TU1JKsUwTqdIB7F0GDGyA3C75/FP6vFyTtsboyEREpIRc1j8qmTZuYMGEC//d//0dSUhLXXnsts2bNKs76zknzqFRShgHLPoR5T0HOSXCGQLex0PJOzbkiIlIOlNo8Kg0aNOCll15i7969TJky5WJ2JVJ0Nhu0u8fsXanVFjJTYNZIs3dFlzGLiFQoxTozbWlTj4rgyoXF78DPYyEnAxxOuPof0GEU+Ditrk5ERApRqjPTiljK7jBDyfDFUO8ayM2En5+H8R1gx69WVyciIhdJQUUqhvC6cOd06DMBgqLg2Bb45EZz3pV0XZ0mIlJeWRpUxo0bR5s2bQgODiYqKopevXqxadMmK0uS8sxmg4RbYeRyaD0UsJnzrrx1GSx8RRPFiYiUQ5YGlQULFjBixAiWLFnCvHnzyM7O5rrrriM9Pd3KsqS8CwiDG1+DofOgegJkJsNPz8FbLWDxf3WTQxGRcqRMDaY9cuQIUVFRLFiwgKuvvvq87TWYVs7LlQvrp5vjVo7vMNeF1ISO/4QW/c2bIIqISKkqt4Npk5OTAQgPDy90e2ZmJikpKR4PkXOyO6DZbebpoJ5vmiElZR98OwreaQNrvzTDjIiIlEllpkfF5XJx0003kZSUxKJFiwptM2bMGJ555pkC69WjIkWWnQErPoZfX4UTeYNsqzWCa56AhjdqwjgRkVLgTY9KmQkq9913H3PmzGHRokXUqlWr0DaZmZlkZma6X6ekpBAbG6ugIt7LTIOl78Hvb0GG2ZNHdHO48iFo1NPsiRERkRJR7oLKyJEj+eabb1i4cCHx8fFFfp/GqMhFO3kcfn8HloyH7LxB3BH14crRkNAXfPwsLU9EpCIqN0HFMAzuv/9+ZsyYwS+//EL9+vW9er+CihSb9GNmD8uy90/1sITUhCvuh8sGgF+QtfWJiFQg5SaoDB8+nMmTJ/PNN9/QoEED9/rQ0FACAgLO+34FFSl2mamwYiIsfhfSDprrAiPMeVnaDIXgGtbWJyJSAZSboGI7y8DFiRMnMmjQoPO+X0FFSkx2BqyZDL+9Ccd3muvsvtC0D1x+H8S0sLI6EZFyrdwElYuloCIlLjcH/pptjmHZs+TU+tpXmIGlYQ8NvBUR8ZKCikhJ2LcSlrwHG6aDK8dcF1ITmveDFndARD1r6xMRKScUVERKUsoBWP6ROR/LycRT62u3NwNLk97gDLauPhGRMk5BRaQ0ZGfApu9g9WTYNh8Ml7neNxAa3QQt+0PclWAvUxNAi4hYTkFFpLSl7Ic1U83QcmzLqfVhtaH5HZBwG0ReYl19IiJliIKKiFUMA/Yuh9WfmzdDzDztflRRTaDxTdD4ZqjWUNP1i0ilpaAiUhZknYC//mde5rx9ARin3fwwov6p0FKjmUKLiFQqCioiZc2JRNg0B/6cBdt+gtysU9vC4szQ0uhmqHmZLncWkQpPQUWkLMtIgc1z4c9vYMuPkHPy1DZnKMS1h7groG4n9baISIWkoCJSXmSlw5Z5Zk/L5h8gK9Vze3AM1L8WLr0e6nbUPYdEpEJQUBEpj3Jz4OBa2PUb7PgVdv4K2SdObXc4oc6VcGk3iO8I1Rqot0VEyiUFFZGKIDsDdi6CLXPNU0VJuzy3B0WZwSX+avMRXlfBRUTKBQUVkYrGMODIJjO0bJ0Pe5ZCToZnm+AYiL8K6lxlBpeqcdbUKiJyHgoqIhVdTibsXWGeHtqx0Jy75fQricCcbK7O1Xnh5UoIrWVNrSIiZ1BQEalssk7A3mXm2JYdC2H/qlM3TsxXpTpEN4foFlCzFdRqA0ERlpQrIpWbgopIZZeZBruXwM6FZnA5sObUvYhOF3EJ1GoLsW3M56hGmsdFREqcgoqIeMpKh0MbzMCy/w/zVNHRzQXb+VWBGgnm/C3RzSG6mTndv8O39GsWkQpLQUVEzu9EojnOZe8y2LMM9q2ErLSC7Rx+ENX4VHCJbmG+9gss9ZJFpGJQUBER77lyzV6WA2vNnpeDa83lzORCGtvMq4qqNTTnc6nWKO+5gSalE5HzUlARkeJhGHB852nBZY35SD9y9veE1T4VXKLyniMbgLNKqZUtImWbgoqIlKz0o3D4Tzjylzm/y5G/zMe5Akxo7bzw0tAMLuHx5g0ZQ2I0gFekklFQERFrpB87FVryH4f/gvTDZ3+P3RfCYqFqHTO4VK1jnlbKfx1QVTPuilQw3vx++5RSTSJSGQRFQFAHqNPBc/2JxLyelz/N56Ob4fguSNoNrmxI3G4+CuMMhaq1zwgy8WaYCY0FX/+S/lYiYiH1qIiIdVy5kHrAHAdzfJf5nLTr1Ou0g+fZgQ2Cowv2wuS/rlID7PYS/hIi4i31qIhI+WB3mFP7h9Yyp/k/U/ZJs9fl+M7Cw0xWGqTuNx+7fy/4focTgmuYYeb055AYz9fO4JL9niJywRRURKTs8g04ddnzmQzDPKV0fCck7SwYZJL2QG5m3vKugu8/nV+VgmEmOBpCok+9rlJDp5mk4tu5CNZPhy5PmuPDygAFFREpn2y2vDExEVCrVcHtuTl5vS0HzdNLKQfM5/zX+c+ZKWbPzLEt5uNcAqrmBZdo895JAWHgH2o+qkSZd7AOiTbb+QZqRl8pX/athM9vg+wT5t/d7i9aXRGgMSoiUtllpkHaoVPhJWV/wTCTegByMrzft81hhpjQWub8MqG1ICDcDDIBVc2gE1AV/POfQ8Gh/38UCxzfCR91PTXFgMMJo1abp0lLgMaoiIgUlbOK+Yiod/Y2hgEZSZ4BJu0QZCSbj5PHIfWQ2YOTcsA85QRg5MLJRPNxcG0R6wkpGGDyX3usOyPo+AXpMm65MCcS4bNbzZBSPcE8xbl3OSx6A254yerqFFRERM7LZjsVDqIanbutYUBultl9np1hhpSkPZC8B1L2maHm5HE4mXTqOSPJPAUF5nNmijmI2Bt238J7aQqsCzPH5DiDPR8+Tu8+TyoGVy58OcA87RlSC/pPM6cP+PQmWDkJrhxdYr0qRaWgIiJSnGw280ffxwkBmGNWqjc5//tys/N6Z5LMAJORVDDUFLbu5HFzLhpXtjmx3rkm1zsXh1/BAON+XcXs6fF4HQx+wZ6v89v4+F1YDVL6ln0AO381/9z6T8sbQF4D4jrArt9g0etww8uWlqigIiJSFjh8ISjSfHjDMMzemzPDizvUnLkuyRw8nJkGmamQnW7uJzfr1Gmqi/4uTvNUlF+QeeWWb4A5uNg38LTlvGe/wDO25732CyxkXd7+fPx1mqs4JG6HH58xl697Dqo3NpdtNuj0L/ikZ16vyoOW9qooqIiIlGc226lQEFrL+/fn5uQFl9RTz/kP9+u0U1dHuV+nQlaq5+uck3n7zISTmcUTegplKyTQBIDvacHIHZLytvv4gc1uvtfuyAtJQacFqiDP135B4BNwWiCyVazJA10umPWA+WcWfzW0Guy5vc5VZaZXRUFFRKQyc/jkjWEJu/h95ebkhZc0M9RknzAn7cs+aS5nnSi4Lv+RdaLguuyTnu/JH6SMYfYEZafDiYsvu8icoebl8IER5pggV475sNlP9fT4OE8t+waYp9QcvmZ7h595vB1+ea/zH355pwv985b9T7222c3vaxhFewbzNE5IzXPfsXzlRPOUj28g9HyrYA+VzQadHoP/62W+NgzLerEUVEREpHg4fE4N4C0JuTlmD0D2SchKPyPYnGVdfgjKycD8Qce8Giv7hNk+K90MVVnpZrv81/k/+qfLTDYfZ7svVVnjH2aGqpzMvGCXdwzAXAfQ5WnzTuaFib8KRq83x61YSEFFRETKB4cPOIJL/pYHhmGGnNPnznHlmON8ThwzH4bLnCfH7jCXs0+aP/45J81AkP/syjYDVm5W3nL+I8vcZ26W+cjJMj8vJ/O055PmvrHl9WYU4RlOXTmWkWQ+zqbOVdD2nnMfC4tDCiioiIiIeLLZzLEvfoGe66tEWVPPhchIhuS8y+Hzx+y4TyXlCYkpF+NuFFREREQqmvxbO1QAlkaphQsX0rNnT2JiYrDZbMycOdPKckRERKSMsTSopKen07x5c959910ryxAREZEyytJTP927d6d79+5WliAiIiJlWLkao5KZmUlmZqb7dUpKioXViIiISEkr+8N9TzNu3DhCQ0Pdj9jYWKtLEhERkRJUroLKY489RnJysvuxZ88eq0sSERGRElSuTv04nU6cTt2KXEREpLIoVz0qIiIiUrlY2qOSlpbG1q1b3a937NjB6tWrCQ8Pp3bt2hZWJiIiImWBpUFlxYoVdO7c2f36oYceAmDgwIFMmjTJoqpERESkrLA0qHTq1AnDKOQOlSIiIiJojIqIiIiUYQoqIiIiUmYpqIiIiEiZpaAiIiIiZVa5mvBNRETKFsMwcBngMgxchoHhXjafDdepbS7DbG8Ap19HYa45tc44bd+c0fbU5556r8uAXJfhUYv5Om/ZOG2by3yd/0HGafsrrA53DacVZmCcanOWtqdqPrXPon7Wmd/P/Z6z1eX+LoV/VoGa87Yl1Aylea1QbDbbmYfXLSM7F1+HHYf97G1KmoKKiAjmD112rovsXBc5uQbZLhfZuQY5uXnPLhfZOeb6nLz1WXltc/La5r8XG/g6bDjsdmyn7TvHdWp/uS7PfeWe/oPvOsty3g/t6UHAVeB9pwJBruuMdnmhwTAgN297RnYuWTkuMnNc5Lhchb7XMAqGjdO3SfnVJCaE/u3iuLlFDEHOU5HAMAxmrdnPi3P+4v4u9enX1rq5zWxGOb4+OCUlhdDQUJKTkwkJCbG6HBE5jWEYZOWaP4CZ2eaPuvmDaP4w5v845q/LX84PBVk5nj/s+T/07iBxWoDIyTXyQsOpNgVCxmlBwnP/ZmAov/8Slh82G9gA+2n/B5+/aOO0/2O3eTwV2u70TgCHzYbNBna7LW/Zht0GDrsNe96205fttlOfZtZUcJ+2M9uc9vm2M+srsL+z7OOML2bjLPstYk2n9mErsD/OqPPM45eZk8tv246RleMCIMDXQZdGUdzYLIaqgb6Mm/MXq/ckAdC8VigzR3Q4Z8+Lt7z5/VaPikglYRgGmTkuTmTlkp6ZQ3pWDumZuWRk53IyK5eMnFwysl2czM4lMztvfba5Ln85M285Iyf3jKDhIjM71wwm2S4y80JJeWazga/djo/Dhq/Djq/Dhk8hr30dNnzyXvs67PjYbRic6kUxDNxd52fuw8d+6r35P6T2vB9Su/3UsvuHN299/o+t47Qf3vwfZrNt3nvPsc/8Z39fB/6+Dpw+dncNjrx2+Z9rO/2z7KdqsZ2+b5sNm91z32fWWZw/dHLxjqdn8fWqvXy+dDc7jqYze+0BZq894N4e6OdgeKd6DLuqrqV/dupRESmjXC6D9KwcUjJySM3IJj0zlxN54eJEVg7peYHjRKa5nL8tP4TkB5ITWbmk5T3nuqz7z93PYcfpY8cv7+F0Pzs8Xvt6/OjnB4H8cHDaj7vdhq+P+drXcZYAYbfj62O29clr45f3nN/G/V6PEGLtOXmR0mQYBmv3JjN77X7+t/YAB1IyuK1VLf5xXQOiQvxL5DO9+f1WUBEpIS6XQWpmDikns0nJyCblZE7eczapGTkF1hVok5lTYqcj/H3tBPn5EOh0EOjrg7+v3f1/1v6+dgLcy6fW+fs68tab4cLpY8fpa8fP4ch7tp/27PB47eewY9cPv0iZlz/Y2NdRshcF69SPSDFzuQyST2ZzLD2To2lZHEvLOm05k2NpWSSeyDIDSF7oSCumoOHnsFPF34cgp4MgPx+CnD4E+jncQSN/XZCfg8DTnqs4HQT6+ZzRzlyn3gIRKYzdbsNO2fr3QUFFKq0TWTkcTc3iaLoZNI6lZXIs/VQIOZaWxdG8dYnpWRd82sTpYyckwJcQf5+8Z1/362B/X0ICfDzWnWpjrvf3dRTzNxcRKT8UVKRCSs/M4UDySQ4kZ3AgKYMDyRkcTDnJ/qQMDiZnsD/5JKkZOV7vNzTAl4gqfkQGOYmo4kdEFT/Cg5xEVvGjaqAfoQGegSPY3wenj4KGiMiFUlCRcikjO5fdiSfYcTSdHUfT2XUsnf1JGe5wUtQQ4vSxE1nFDBoRVZxEBJnPkXkhJCIvkERWcVI10A8/H03mLCJSmhRUpMzKdRnsO36S7UfT2HE0nZ1H09meF0z2JZ087/iPYH8fokP9iQ4NIDrUnxqh/sSEBlAj1N/9uorTR5dMioiUYQoqYinDMDicmunuGdlxNJ3tR9LZcTSNPYknyco9+1wcwU4f4qsFER8ZRFxEELXCAogOyw8hAVRx6q+3iEh5p3/JpdQknchiw/4U1u9LZuOBFLYeNntKTmTlnvU9fj524iOCqBMZSHxkFepGBrnDSUSQn3pDREQqOAUVKRGJ6Vms25fM+n3JrNubzLp9yexLOlloW7sNYsMDiY8MKvCICQ3Q/BsiIpWYgopctMT0LNbuTTJDyb5k1u9LOWsoiYsIpElMCE1iQrm0ejB1qwURWzVQg1RFRKRQCiriFcMw2HXsBMt3JrJi53GW70pk+5H0QtvWjQyiac1QmtYMoWnNUJrEhBIa4FvKFYuISHmmoCLnlJPr4s8DqSzbmciKnYks33mco2mZBdrVrRZEQs1QEmqG5oWSEIL9FUpEROTiKKiIB8Mw2HI4jYWbj7Bwy1FW7EwsMNjVz2GnWa1QWtcJp02dqrSKq0pYoJ9FFYuISEWmoCKkZmTz29Zj/LLpMAs2H+FAcobH9hB/H1rXCad1naq0qRNOQs1QTesuIiKlQkGlEjIMg82H0vhl02F+2XSE5TsTyTntPjZOHztt48PpeGk1OlwSSYPqwbryRkRELKGgUknkugxW7jrO3A0HmbvhIHuPe16VEx8ZRMdLq9GpQTUurxuhHhMRESkTFFQqsMycXH7fdowfNhxk3sZDHE3Lcm9z+thpXy+CTpdWo1ODKOpEBllYqYiISOEUVCqY9Mwcftl0hLkbDvLzX4dJzTx1c74Qfx+6NqrOdU1q0PHSagT4qddERETKNgWVCiAxPYsfNx5i7oaD/Lr1KFk5p+6PExXs5Lom1bm+STTt6obj69DEaiIiUn4oqJRT+5JO8kPeeJNlOxI5bSwsdSIC6dakBt2a1qBFrTANhBURkXJLQaUc2Xo4lbkbDvH9+oOs25fssa1xdAjXN61BtyY1uLR6Fd2sT0REKgQFlTLMMAzW7E12X6lz+lT1Nhu0iQvnuibV6dakBrHhgRZWKiIiUjIUVMqY5JPZLN52lAWbj/LLpsMek6/5Omx0uCSSbk1q0LVRdaoFOy2sVEREpOQpqFgs12WwZm8Sv24+ysItR1i9J4nc0wacBPo56NwgiuuaVKdzwyhCdP8cERGpRBRUStmR1Ew2Hkhh1a7jrNp9nNV7kkjNyPFoU7daEFfXr8bVl0ZyRb1ITb4mIiKVloJKCcjOdbH3+El2Hk1nx9F0dh5LZ9uRNP46kMqx9KwC7YP9fbjykkiuvrQaV9WPpFZVjTcREREBBZUL5nIZHEjJYMeRdHYcTWPH0RN5z+nsPX7S4945p7PZoE5EEM1rhXJZXFUuq12VhjWC8dH8JiIiIgUoqBTRweQMFm8/yu9bj7FuXzI7jqaTedrEamfy97VTJyLIfEQGUTcyiAY1grm0erBmhBURESkiBZXz+HXLEV6eu4m1e5MLbPN12KgdHkh8ZBDxkWYgyV+uHuyvidZEREQuUpkIKu+++y4vv/wyBw8epHnz5rz99tu0bdvW0pr+OpjCuO/+YsHmI4B5yiahZijt60XQtk44l0RVoWZYgE7ZiIiIlCDLg8oXX3zBQw89xHvvvUe7du1444036NatG5s2bSIqKsqSmib+toPnZm/EZZi9JndeHseIzpcQWUXzloiIiJQmy7sDXnvtNe6++24GDx5M48aNee+99wgMDOTjjz+2rKa28eEA9EiI5seHOvJ0zyYKKSIiIhawtEclKyuLlStX8thjj7nX2e12unbtyuLFiwu0z8zMJDMz0/06JSWlROpqEhPKz//oRFxEUInsX0RERIrG0h6Vo0ePkpubS/Xq1T3WV69enYMHDxZoP27cOEJDQ92P2NjYEqtNIUVERMR6lp/68cZjjz1GcnKy+7Fnzx6rSxIREZESZOmpn8jISBwOB4cOHfJYf+jQIWrUqFGgvdPpxOnUWBEREZHKwtIeFT8/P1q1asX8+fPd61wuF/Pnz6d9+/YWViYiIiJlgeWXJz/00EMMHDiQ1q1b07ZtW9544w3S09MZPHiw1aWJiIiIxSwPKn/72984cuQITz31FAcPHqRFixZ8//33BQbYioiISOVjMwyj8LvnlQMpKSmEhoaSnJxMSEiI1eWIiIhIEXjz+12urvoRERGRykVBRURERMosBRUREREpsxRUREREpMxSUBEREZEyS0FFREREyiwFFRERESmzLJ/w7WLkTwGTkpJicSUiIiJSVPm/20WZyq1cB5XU1FQAYmNjLa5EREREvJWamkpoaOg525TrmWldLhf79+8nODgYm81WrPtOSUkhNjaWPXv2VNpZb3UMdAzy6TjoGICOQT4dh4s/BoZhkJqaSkxMDHb7uUehlOseFbvdTq1atUr0M0JCQirtX8R8OgY6Bvl0HHQMQMcgn47DxR2D8/Wk5NNgWhERESmzFFRERESkzFJQOQun08nTTz+N0+m0uhTL6BjoGOTTcdAxAB2DfDoOpXsMyvVgWhEREanY1KMiIiIiZZaCioiIiJRZCioiIiJSZimoiIiISJmloFKId999lzp16uDv70+7du1YtmyZ1SWVmHHjxtGmTRuCg4OJioqiV69ebNq0yaNNRkYGI0aMICIigipVqtCnTx8OHTpkUcUl74UXXsBmszF69Gj3uspyDPbt28edd95JREQEAQEBJCQksGLFCvd2wzB46qmniI6OJiAggK5du7JlyxYLKy5eubm5PPnkk8THxxMQEEC9evV47rnnPO5HUhGPwcKFC+nZsycxMTHYbDZmzpzpsb0o3zkxMZH+/fsTEhJCWFgYQ4cOJS0trRS/xcU51zHIzs7m0UcfJSEhgaCgIGJiYhgwYAD79+/32Ed5PwZw/r8Lp7v33nux2Wy88cYbHuuL+zgoqJzhiy++4KGHHuLpp59m1apVNG/enG7dunH48GGrSysRCxYsYMSIESxZsoR58+aRnZ3NddddR3p6urvNgw8+yLfffsu0adNYsGAB+/fv55ZbbrGw6pKzfPly3n//fZo1a+axvjIcg+PHj9OhQwd8fX2ZM2cOGzdu5NVXX6Vq1aruNi+99BJvvfUW7733HkuXLiUoKIhu3bqRkZFhYeXF58UXX2T8+PG88847/Pnnn7z44ou89NJLvP322+42FfEYpKen07x5c959991CtxflO/fv358NGzYwb948Zs+ezcKFC7nnnntK6ytctHMdgxMnTrBq1SqefPJJVq1axfTp09m0aRM33XSTR7vyfgzg/H8X8s2YMYMlS5YQExNTYFuxHwdDPLRt29YYMWKE+3Vubq4RExNjjBs3zsKqSs/hw4cNwFiwYIFhGIaRlJRk+Pr6GtOmTXO3+fPPPw3AWLx4sVVllojU1FSjfv36xrx584yOHTsao0aNMgyj8hyDRx991LjyyivPut3lchk1atQwXn75Zfe6pKQkw+l0GlOmTCmNEktcjx49jCFDhnisu+WWW4z+/fsbhlE5jgFgzJgxw/26KN9548aNBmAsX77c3WbOnDmGzWYz9u3bV2q1F5czj0Fhli1bZgDGrl27DMOoeMfAMM5+HPbu3WvUrFnTWL9+vREXF2e8/vrr7m0lcRzUo3KarKwsVq5cSdeuXd3r7HY7Xbt2ZfHixRZWVnqSk5MBCA8PB2DlypVkZ2d7HJOGDRtSu3btCndMRowYQY8ePTy+K1SeYzBr1ixat27NbbfdRlRUFC1btuTDDz90b9+xYwcHDx70OA6hoaG0a9euwhyHK664gvnz57N582YA1qxZw6JFi+jevTtQOY7BmYrynRcvXkxYWBitW7d2t+natSt2u52lS5eWes2lITk5GZvNRlhYGFB5joHL5eKuu+7ikUceoUmTJgW2l8RxKNc3JSxuR48eJTc3l+rVq3usr169On/99ZdFVZUel8vF6NGj6dChA02bNgXg4MGD+Pn5uf9jzFe9enUOHjxoQZUlY+rUqaxatYrly5cX2FZZjsH27dsZP348Dz30EI8//jjLly/ngQcewM/Pj4EDB7q/a2H/fVSU4/Cvf/2LlJQUGjZsiMPhIDc3l+eff57+/fsDVIpjcKaifOeDBw8SFRXlsd3Hx4fw8PAKeVwyMjJ49NFH6devn/uGfJXlGLz44ov4+PjwwAMPFLq9JI6Dgoq4jRgxgvXr17No0SKrSylVe/bsYdSoUcybNw9/f3+ry7GMy+WidevWjB07FoCWLVuyfv163nvvPQYOHGhxdaXjyy+/5PPPP2fy5Mk0adKE1atXM3r0aGJiYirNMZBzy87Opm/fvhiGwfjx460up1StXLmSN998k1WrVmGz2Urtc3Xq5zSRkZE4HI4CV3McOnSIGjVqWFRV6Rg5ciSzZ8/m559/platWu71NWrUICsri6SkJI/2FemYrFy5ksOHD3PZZZfh4+ODj48PCxYs4K233sLHx4fq1atX+GMAEB0dTePGjT3WNWrUiN27dwO4v2tF/u/jkUce4V//+he33347CQkJ3HXXXTz44IOMGzcOqBzH4ExF+c41atQocMFBTk4OiYmJFeq45IeUXbt2MW/ePHdvClSOY/Drr79y+PBhateu7f63cteuXTz88MPUqVMHKJnjoKByGj8/P1q1asX8+fPd61wuF/Pnz6d9+/YWVlZyDMNg5MiRzJgxg59++on4+HiP7a1atcLX19fjmGzatIndu3dXmGPSpUsX1q1bx+rVq92P1q1b079/f/dyRT8GAB06dChwafrmzZuJi4sDID4+nho1angch5SUFJYuXVphjsOJEyew2z3/WXQ4HLhcLqByHIMzFeU7t2/fnqSkJFauXOlu89NPP+FyuWjXrl2p11wS8kPKli1b+PHHH4mIiPDYXhmOwV133cXatWs9/q2MiYnhkUceYe7cuUAJHYcLGoJbgU2dOtVwOp3GpEmTjI0bNxr33HOPERYWZhw8eNDq0krEfffdZ4SGhhq//PKLceDAAffjxIkT7jb33nuvUbt2beOnn34yVqxYYbRv395o3769hVWXvNOv+jGMynEMli1bZvj4+BjPP/+8sWXLFuPzzz83AgMDjc8++8zd5oUXXjDCwsKMb775xli7dq1x8803G/Hx8cbJkyctrLz4DBw40KhZs6Yxe/ZsY8eOHcb06dONyMhI45///Ke7TUU8BqmpqcYff/xh/PHHHwZgvPbaa8Yff/zhvqKlKN/5+uuvN1q2bGksXbrUWLRokVG/fn2jX79+Vn0lr53rGGRlZRk33XSTUatWLWP16tUe/1ZmZma691Hej4FhnP/vwpnOvOrHMIr/OCioFOLtt982ateubfj5+Rlt27Y1lixZYnVJJQYo9DFx4kR3m5MnTxrDhw83qlatagQGBhq9e/c2Dhw4YF3RpeDMoFJZjsG3335rNG3a1HA6nUbDhg2NDz74wGO7y+UynnzySaN69eqG0+k0unTpYmzatMmiaotfSkqKMWrUKKN27dqGv7+/UbduXeOJJ57w+DGqiMfg559/LvTfgYEDBxqGUbTvfOzYMaNfv35GlSpVjJCQEGPw4MFGamqqBd/mwpzrGOzYseOs/1b+/PPP7n2U92NgGOf/u3CmwoJKcR8Hm2GcNuWiiIiISBmiMSoiIiJSZimoiIiISJmloCIiIiJlloKKiIiIlFkKKiIiIlJmKaiIiIhImaWgIiIiImWWgoqIVCg2m42ZM2daXYaIFBMFFREpNoMGDcJmsxV4XH/99VaXJiLllI/VBYhIxXL99dczceJEj3VOp9OiakSkvFOPiogUK6fTSY0aNTweVatWBczTMuPHj6d79+4EBARQt25dvvrqK4/3r1u3jmuuuYaAgAAiIiK45557SEtL82jz8ccf06RJE5xOJ9HR0YwcOdJj+9GjR+nduzeBgYHUr1+fWbNmleyXFpESo6AiIqXqySefpE+fPqxZs4b+/ftz++238+effwKQnp5Ot27dqFq1KsuXL2fatGn8+OOPHkFk/PjxjBgxgnvuuYd169Yxa9YsLrnkEo/PeOaZZ+jbty9r167lhhtuoH///iQmJpbq9xSRYnLBtzMUETnDwIEDDYfDYQQFBXk8nn/+ecMwzLt133vvvR7vadeunXHfffcZhmEYH3zwgVG1alUjLS3Nvf1///ufYbfbjYMHDxqGYRgxMTHGE088cdYaAOPf//63+3VaWpoBGHPmzCm27ykipUdjVESkWHXu3Jnx48d7rAsPD3cvt2/f3mNb+/btWb16NQB//vknzZs3JygoyL29Q4cOuFwuNm3ahM1mY//+/XTp0uWcNTRr1sy9HBQUREhICIcPH77QryQiFlJQEZFiFRQUVOBUTHEJCAgoUjtfX1+P1zabDZfLVRIliUgJ0xgVESlVS5YsKfC6UaNGADRq1Ig1a9aQnp7u3v7bb79ht9tp0KABwcHB1KlTh/nz55dqzSJiHfWoiEixyszM5ODBgx7rfHx8iIyMBGDatGm0bt2aK6+8ks8//5xly5YxYcIEAPr378/TTz/NwIEDGTNmDEeOHOH+++/nrrvuonr16gCMGTOGe++9l6ioKLp3705qaiq//fYb999/f+l+UREpFQoqIlKsvv/+e6Kjoz3WNWjQgL/++gswr8iZOnUqw4cPJzo6milTptC4cWMAAgMDmTt3LqNGjaJNmzYEBgbSp08fXnvtNfe+Bg4cSEZGBq+//jr/+Mc/iIyM5NZbby29LygipcpmGIZhdREiUjnYbDZmzJhBr169rC5FRMoJjVERERGRMktBRURERMosjVERkVKjM80i4i31qIiIiEiZpaAiIiIiZZaCioiIiJRZCioiIiJSZimoiIiISJmloCIiIiJlloKKiIiIlFkKKiIiIlJmKaiIiIhImfX/+EYJXKQ51ZgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('Model accuracy and loss')\n",
    "    plt.ylabel('Accuracy and Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Accuracy', 'Loss'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(model.history)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"/content/drive/MyDrive/Colab Notebooks/shakespeare-\" + model.name + \"-embeddings.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PoYhGqQ0uHw"
   },
   "source": [
    "## Perform a prediction or two\n",
    "\n",
    "Finally we can run predictions based on seed inputs, to see how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "DG9m4rOy00mz",
    "outputId": "7cf8740e-5bd2-41bf-fb8c-6922a5252a68",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My kingdom for you are old and say not be true love you fortunes of good duke of\n",
      "My daughter is to know you and my sister nothing my lord friend being nothing now of me\n",
      "If only I were a soldier he is differency between a silver and it from the king henry man\n"
     ]
    }
   ],
   "source": [
    "# Load the model, and the tokenizer\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "model = load_model(\"/content/drive/MyDrive/Colab Notebooks/shakespeare-\" + model.name + \"-embeddings.keras\")\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/shakespeare-tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "# Predict next words based on an input sequence\n",
    "\n",
    "def predict_next_words(seed_text, next_words, model, max_seq_len, tokenizer):\n",
    "  for _ in range(next_words):\n",
    "      token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "      token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
    "      predicted = model.predict(token_list, verbose=0)\n",
    "      predicted_class = np.argmax(predicted, axis=-1)\n",
    "      output_word = \"\"\n",
    "      for word, index in tokenizer.word_index.items():\n",
    "          if index == predicted_class:\n",
    "              output_word = word\n",
    "              break\n",
    "      seed_text += \" \" + output_word\n",
    "  return seed_text\n",
    "\n",
    "seed_texts = [\"My kingdom for\", \"My daughter is\", \"If only I were\"]\n",
    "for seed_text in seed_texts:\n",
    "  next_words = 15\n",
    "  print(predict_next_words(seed_text, next_words, model, max_seq_len, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-KVQTmbRTvk"
   },
   "source": [
    "## Final remarks\n",
    "\n",
    "We could demonstrate very simple, contextual text generation based on a primitive Recurrent Neural Network (RNN). Despite being far from the capabilities of a large language model, we can still illustrate the foundational principles that enable these models to generate text based on learned patterns from a given dataset. Through this simplified example, we aim to shed light on how RNNs can grasp basic language structures and produce text that, while not as sophisticated or coherent as that generated by more advanced models like GPT or BERT, still reflects the initial steps towards understanding and replicating human language.\n",
    "\n",
    "Our basic RNN, although limited, can be trained on a small dataset of text to learn sequences of words or characters. After training, it can generate new sequences of text by predicting the next word or character based on a given seed text. This process starts with the model receiving an initial input (the seed), processing it to make a prediction, and then feeding the prediction back into the model as part of the next input sequence."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "nikola": {
   "author": "Pedro Leitao",
   "category": "Machine Learning",
   "date": "2024-03-18 16:01:00",
   "description": "",
   "link": "",
   "slug": "shakespeare-embeddings",
   "tags": "experiments, machine-learning, tensorflow, keras",
   "title": "An embeddings based Recurrent Neural Network (RNN) to predict Shakespearean text",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
