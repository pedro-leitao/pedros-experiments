
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>An Exploration of Language Prediction: A Simplified Model for Shakespearean Text Generation &#8212; Pedro&#39;s experiments</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'experiments/shakespeare-embeddings';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Basics of word vectors" href="gensim.html" />
    <link rel="prev" title="Sentiment Classification of Financial Texts using Neural Networks" href="sentiment-partI.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo-new.png" class="logo__image only-light" alt="Pedro's experiments - Home"/>
    <script>document.write(`<img src="../_static/logo-new.png" class="logo__image only-dark" alt="Pedro's experiments - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Pedro’s experiments
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">How to's</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../howtos/apple-silicon-ml.html">Setting up a Machine Learning environment on Apple Silicon</a></li>
<li class="toctree-l1"><a class="reference internal" href="../howtos/distributed-training.html">Distributed training with Keras &amp; Tensorflow</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Experiments</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="instance_vs_model_learning.html">Instance vs Model Learning</a></li>


<li class="toctree-l1"><a class="reference internal" href="ml-pipeline-sklearn.html">A Wine Quality Prediction Experiment with SKLearn Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="sentiment-partI.html">Sentiment Classification of Financial Texts using Neural Networks</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">An Exploration of Language Prediction: A Simplified Model for Shakespearean Text Generation</a></li>

<li class="toctree-l1"><a class="reference internal" href="gensim.html">Basics of word vectors</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/experiments/shakespeare-embeddings.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>An Exploration of Language Prediction: A Simplified Model for Shakespearean Text Generation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">An Exploration of Language Prediction: A Simplified Model for Shakespearean Text Generation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mount-google-drive-into-colab-and-read-our-dataset">Mount Google Drive into Colab and read our dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#text-tokenization">Text Tokenization</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#a-function-to-create-n-grams">A function to create n-grams</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-example-n-gram-output">An example n-gram output</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-the-model">Defining the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#padding-sequences">Padding sequences</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#verifying-sequences">Verifying sequences</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#early-stopping-and-instantiating-the-model">Early stopping and instantiating the model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#early-stopping">Early Stopping</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lstm-model-creation">LSTM Model Creation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plotting-the-training-history-and-saving-the-model">Plotting the training history and saving the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#perform-a-prediction-or-two">Perform a prediction or two</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-remarks">Final remarks</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="an-exploration-of-language-prediction-a-simplified-model-for-shakespearean-text-generation">
<h1>An Exploration of Language Prediction: A Simplified Model for Shakespearean Text Generation<a class="headerlink" href="#an-exploration-of-language-prediction-a-simplified-model-for-shakespearean-text-generation" title="Link to this heading">#</a></h1>
<p>The recent advancements in large language models have captivated many, leading to curiosity about the underlying mechanisms of such sophisticated technology. In this context, we propose to explore a fundamental model aimed at predicting Shakespearean text, demonstrating the basic principles of language model training in a simplified manner.</p>
<p>The model is constructed using the following architecture:</p>
<ul class="simple">
<li><p><strong>Embedding Layer</strong>: This layer initializes with the vocabulary size, indicating the range of unique words the model needs to recognize, and an output dimension of 150, designed to create a dense representation of word meanings in a 150-dimensional vector space. The input shape parameter is set based on the sequence length minus one, preparing the model to process sequences of text one word at a time.</p></li>
<li><p><strong>LSTM Layer</strong>: A Long Short-Term Memory (LSTM) layer with 150 units is employed, utilizing a ‘tanh’ activation function. This choice is due to LSTM’s capability to remember and learn from long sequences of data, making it particularly suited for text prediction tasks where the context and order of words are crucial.</p></li>
<li><p><strong>Dense Layer</strong>: The final layer is a Dense layer with a softmax activation function. It outputs a probability distribution over the vocabulary, indicating the model’s prediction for the next word in the sequence.</p></li>
<li><p><strong>Optimizer</strong>: The Adam optimizer is used with a learning rate of 0.0005, a modification from the default setting intended to mitigate the risk of exploding gradients, thus ensuring more stable training.</p></li>
</ul>
<p>The model is compiled with a loss function of ‘sparse_categorical_crossentropy’ and tracks accuracy as a performance metric. This configuration offers a balance between computational efficiency and the capacity to capture the nuanced structure of Shakespearean English.</p>
<p>This experiment serves as an illustrative example of how even simplistic models can approach the task of language prediction, providing insights into the methodologies that underpin more advanced language processing systems. Through such exploration, we gain a deeper understanding of the challenges and potential of machine learning in linguistic applications.</p>
<section id="mount-google-drive-into-colab-and-read-our-dataset">
<h2>Mount Google Drive into Colab and read our dataset<a class="headerlink" href="#mount-google-drive-into-colab-and-read-our-dataset" title="Link to this heading">#</a></h2>
<p>As this model can take quite a while to train (many hours or days depending on your hardware), we will be running this on <a class="reference external" href="https://colab.google/">Google Colab</a>, where we can access quite a bit of cloud GPU time for relatively little money.</p>
<p>We will be saving all results on Google Drive for long term storage, as sessions do not keep any results when they are shutdown.</p>
<p>As the dataset, we will be using <a class="reference external" href="https://www.kaggle.com/datasets/kingburrito666/shakespeare-plays">~100,000 lines from Shakespeare plays</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
<span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s1">&#39;/content/drive/&#39;</span><span class="p">,</span> <span class="n">force_remount</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s1">&#39;/content/drive/&#39;</span><span class="p">,</span> <span class="n">force_remount</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;google.colab&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>

<span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[^a-zA-Z ]&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

<span class="c1"># Read all lines from alllines.txt which are in quotes, and add to an array.</span>
<span class="k">def</span> <span class="nf">read_lines</span><span class="p">():</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;/content/drive/MyDrive/Colab Notebooks/alllines.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;^&quot;.*&quot;$&#39;</span><span class="p">,</span> <span class="n">line</span><span class="p">):</span>
                <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">preprocess</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()))</span>
    <span class="k">return</span> <span class="n">lines</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="text-tokenization">
<h2>Text Tokenization<a class="headerlink" href="#text-tokenization" title="Link to this heading">#</a></h2>
<p>We start by tokenizing all the text in each line, and then saving the resulting tokenizer. As we will need it at later stages for prediction. We tokenize with the inbuilt <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer">Keras pre-processing tokenizer</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use Keras to tokenize the lines.</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>

<span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">lines</span><span class="p">):</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">lines</span><span class="p">),</span> <span class="n">tokenizer</span>

<span class="c1"># Read lines and tokenize them.</span>
<span class="n">lines</span> <span class="o">=</span> <span class="n">read_lines</span><span class="p">()</span>
<span class="n">sequences</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenize</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span>

<span class="c1"># Save the tokenizer</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;/content/drive/MyDrive/Colab Notebooks/shakespeare-tokenizer.pickle&#39;</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">handle</span><span class="p">:</span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">handle</span><span class="p">,</span> <span class="n">protocol</span><span class="o">=</span><span class="n">pickle</span><span class="o">.</span><span class="n">HIGHEST_PROTOCOL</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="a-function-to-create-n-grams">
<h1>A function to create n-grams<a class="headerlink" href="#a-function-to-create-n-grams" title="Link to this heading">#</a></h1>
<p>For our model to be able to learn, we need to show it sequences of words (or in our case, integer token representations). We therefore need an n-gram generator for each of our sequences (or lines). This specific n-gram generator produces all possible n-grams from our sequence list (i.e., all lines in the dataset), for each <code class="docutils literal notranslate"><span class="pre">every_nth</span></code> line.</p>
<p>n-grams are contiguous sequences of n items from a given sample of text or speech. In the context of natural language processing (NLP), these items are typically words, though they can also be characters or subword units. The concept of n-grams allows models to capture the context up to n-1 words, providing a foundation for predicting subsequent elements in a sequence. This approach is crucial for understanding the linguistic structure and for tasks such as text prediction, where the likelihood of a word’s occurrence may depend significantly on the preceding words.</p>
<p>For instance, consider the sentence: “The quick brown fox jumps over the lazy dog.” If we were to generate 2-grams (also known as bigrams) for this sentence, we would produce a list of consecutive word pairs: [“The quick”, “quick brown”, “brown fox”, “fox jumps”, “jumps over”, “over the”, “the lazy”, “lazy dog”]. Similarly, 3-grams (trigrams) would yield: [“The quick brown”, “quick brown fox”, “brown fox jumps”, “fox jumps over”, “jumps over the”, “over the lazy”, “the lazy dog”]. By examining these n-grams, a model learns the common patterns of word sequences in a language, which aids in predicting what word naturally follows a given sequence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_ngrams</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">every_nth</span><span class="p">):</span>
    <span class="n">ngrams</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">token_list</span> <span class="ow">in</span> <span class="n">sequences</span><span class="p">[::</span><span class="n">every_nth</span><span class="p">]:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_list</span><span class="p">)):</span>
            <span class="n">ngram_sequence</span> <span class="o">=</span> <span class="n">token_list</span><span class="p">[:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">ngrams</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ngram_sequence</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ngrams</span>
</pre></div>
</div>
</div>
</div>
<section id="an-example-n-gram-output">
<h2>An example n-gram output<a class="headerlink" href="#an-example-n-gram-output" title="Link to this heading">#</a></h2>
<p>Here’s an example of the generated n-grams for the first five n-grams produced (integers because we are looking at token indices rather than the actual tokens).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ngrams</span> <span class="o">=</span> <span class="n">create_ngrams</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ngrams</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[307, 3], [194, 39], [194, 39, 6], [194, 39, 6, 112], [194, 39, 6, 112, 16]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="defining-the-model">
<h2>Defining the model<a class="headerlink" href="#defining-the-model" title="Link to this heading">#</a></h2>
<p>We now define the model which aims to harness the power of LSTM (Long Short-Term Memory) networks for the purpose of text prediction, particularly focusing on generating text that mimics the style of Shakespearean English. The model’s architecture is designed to understand and replicate the intricate patterns of language use found in Shakespeare’s works.</p>
<p>The model begins with an <code class="docutils literal notranslate"><span class="pre">Embedding</span></code> layer, which is essential for converting the integer-encoded vocabulary into dense vectors of fixed size. This layer acts as a lookup table that allows the model to learn a dense representation of words, capturing semantic meanings that are vital for understanding the context of different words in sentences. The <code class="docutils literal notranslate"><span class="pre">output_dim</span></code> parameter is set to 150, indicating that each word is represented by a 150-dimensional vector. The <code class="docutils literal notranslate"><span class="pre">input_shape</span></code> is determined by <code class="docutils literal notranslate"><span class="pre">seq_len-1</span></code>, preparing the model to process sequences of a specific length, minus one for the word to be predicted.</p>
<p>Following the <code class="docutils literal notranslate"><span class="pre">Embedding</span></code> layer, we incorporate an <code class="docutils literal notranslate"><span class="pre">LSTM</span></code> layer with 150 units. LSTM layers are crucial for sequence prediction problems because they can maintain long-term dependencies, overcoming the vanishing gradient problem common in traditional recurrent neural networks (RNNs). The <code class="docutils literal notranslate"><span class="pre">activation</span></code> function used is <code class="docutils literal notranslate"><span class="pre">tanh</span></code>, chosen for its efficacy in LSTMs over alternatives like <code class="docutils literal notranslate"><span class="pre">relu</span></code>, providing a balance between linear and non-linear transformations.</p>
<p>The final layer is a <code class="docutils literal notranslate"><span class="pre">Dense</span></code> layer with a <code class="docutils literal notranslate"><span class="pre">softmax</span></code> activation function. This layer outputs a probability distribution over the entire vocabulary for the next word prediction, based on the context provided by the input sequence. The size of this layer matches the vocabulary size, ensuring each word in the vocabulary can be predicted.</p>
<p>An <code class="docutils literal notranslate"><span class="pre">Adam</span></code> optimizer with a learning rate of <code class="docutils literal notranslate"><span class="pre">0.0005</span></code> is used to compile the model. This reduced learning rate is a deliberate choice to mitigate the risk of exploding gradients, a common issue in training deep neural networks, especially on complex sequence data. The model uses <code class="docutils literal notranslate"><span class="pre">sparse_categorical_crossentropy</span></code> as the loss function, appropriate for this multi-class classification problem, and tracks <code class="docutils literal notranslate"><span class="pre">accuracy</span></code> as a metric to evaluate its performance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a model with an embeddings layer.</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers.schedules</span> <span class="kn">import</span> <span class="n">ExponentialDecay</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.regularizers</span> <span class="kn">import</span> <span class="n">l2</span>

<span class="k">def</span> <span class="nf">create_lstm_model</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;LSTM&quot;</span><span class="p">)</span>
    <span class="c1"># In Keras3, Embedding doesn&#39;t have input_length, instead use input_shape.</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">seq_len</span><span class="o">-</span><span class="mi">1</span><span class="p">,)))</span>
    <span class="c1"># Explicitly set the activation function to &#39;tanh&#39;, as &#39;relu&#39; is not recommended for LSTM layers</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>

    <span class="c1"># Adjust learning rate to 1/4 of its default value, 0.001 -&gt; 0.00050 - this is to reduce the chance of exploding gradients</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;sparse_categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="padding-sequences">
<h2>Padding sequences<a class="headerlink" href="#padding-sequences" title="Link to this heading">#</a></h2>
<p>Not all sequences (lines of text) are the same size, but the model requires a constant size for its input. This discrepancy poses a challenge in training neural network models, especially those dealing with language processing. To address this issue, a technique known as padding is employed to standardize the length of sequences.</p>
<p>Padding involves altering the sequences to ensure they all have the same length. This is achieved by adding a special type of token, often referred to as a “pad token,” to the sequences that are shorter than the required length. Conversely, sequences that exceed the desired length can be truncated to fit the model’s requirements. The choice of where to pad (beginning or end of the sequence) or truncate (beginning or end) can depend on the specific application and the nature of the data.</p>
<p>In practice, padding is applied before feeding the sequences into the model, ensuring that every input sequence has the same shape. This uniformity is crucial for the model to perform batch processing, a computational efficiency technique where multiple sequences are processed simultaneously. Batch processing significantly speeds up the training and inference phases by leveraging parallel computation capabilities.</p>
<p>For instance, consider a scenario where we are training a model on a dataset of sentences with varying lengths, and our model requires each input sequence to have a length of 10 words. A sentence with 8 words would be padded with 2 pad tokens at the end (or the beginning, as per the chosen strategy), while a sentence with 12 words might be truncated to the first (or last) 10 words.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">max_seq_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">ngrams</span><span class="p">])</span>  <span class="c1"># Determine the max length after forming n-grams</span>
<span class="n">padded_sequences</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pad_sequences</span><span class="p">(</span><span class="n">ngrams</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;pre&#39;</span><span class="p">))</span>
<span class="n">input_sequences</span> <span class="o">=</span> <span class="n">padded_sequences</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">target_words</span> <span class="o">=</span> <span class="n">padded_sequences</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="verifying-sequences">
<h2>Verifying sequences<a class="headerlink" href="#verifying-sequences" title="Link to this heading">#</a></h2>
<p>Once all sequences have been appropriately padded to meet the model’s requirements for uniform input size, it becomes crucial to verify the integrity and appropriateness of these modified sequences. Ensuring the correctness of padded sequences involves checking that there are no empty sequences and that sequences are not solely composed of zeros, which represent the padding tokens. This verification step is fundamental to ensuring the model receives meaningful and accurate data for training.</p>
<p>The presence of sequences filled exclusively with pad tokens (zeros) or empty sequences could significantly impact the model’s learning process. Such sequences do not provide any valuable information or context that the model can learn from, potentially skewing the training process or leading to biases in the model’s predictions. For instance, a model trained on a dataset with a high number of zero-only sequences might become inclined to predict pad tokens more frequently, reducing its overall effectiveness in generating or classifying text.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check that the input sequences are not empty, that are not all zeros, and that there are no NaN values. Print an error if they do.</span>
<span class="k">def</span> <span class="nf">check_input_sequences</span><span class="p">(</span><span class="n">input_sequences</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Checking input sequences...&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">sequence</span> <span class="ow">in</span> <span class="n">input_sequences</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">sequence</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error: input sequence contains an empty sequence or a sequence of only zeros.&quot;</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">sequence</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error: input sequence contains NaN or Inf values.&quot;</span><span class="p">)</span>
                <span class="k">return</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input sequences are valid.&quot;</span><span class="p">)</span>

<span class="n">check_input_sequences</span><span class="p">(</span><span class="n">input_sequences</span><span class="p">)</span>

<span class="c1"># Check that the target words are not empty, that are not all zeros, and that there are no NaN values. Print an error if they do.</span>
<span class="k">def</span> <span class="nf">check_target_words</span><span class="p">(</span><span class="n">target_words</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Checking target words...&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">target_words</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">target_words</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">target_words</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error: target words contain an empty sequence or a sequence of only zeros.&quot;</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">target_words</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error: target words contain 0, NaN or Inf values.&quot;</span><span class="p">)</span>
            <span class="k">return</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Target words are valid.&quot;</span><span class="p">)</span>

<span class="n">check_target_words</span><span class="p">(</span><span class="n">target_words</span><span class="p">)</span>

<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Max sequence length:&quot;</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x size:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_sequences</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y size:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">target_words</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary size:&quot;</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Example input sequence:&quot;</span><span class="p">,</span> <span class="n">input_sequences</span><span class="p">[</span><span class="mi">7</span><span class="p">:</span><span class="mi">9</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Example target word:&quot;</span><span class="p">,</span> <span class="n">target_words</span><span class="p">[</span><span class="mi">7</span><span class="p">:</span><span class="mi">9</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Checking input sequences...
Input sequences are valid.
Checking target words...
Target words are valid.
Max sequence length: 163
x size: 175704
y size: 175704
Vocabulary size: 27390
Example input sequence: [[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0  194   39    6  112   16 3956  197]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
   194   39    6  112   16 3956  197    4]]
Example target word: [   4 9703]
</pre></div>
</div>
</div>
</div>
</section>
<section id="early-stopping-and-instantiating-the-model">
<h2>Early stopping and instantiating the model<a class="headerlink" href="#early-stopping-and-instantiating-the-model" title="Link to this heading">#</a></h2>
<p>Overfitting occurs when a model learns the training data too well, capturing noise and outliers in the process, which ultimately leads to poor performance on unseen data. Let’s break down the early stopping aspect and then briefly touch on the model creation:</p>
<section id="early-stopping">
<h3>Early Stopping<a class="headerlink" href="#early-stopping" title="Link to this heading">#</a></h3>
<p><strong>Early stopping</strong> is a form of regularization used to avoid overfitting by halting the training process if the model’s performance stops improving on a held-out validation dataset. Here’s how the provided <code class="docutils literal notranslate"><span class="pre">EarlyStopping</span></code> callback configuration works:</p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">monitor='loss'</span></code></strong>: This specifies that the callback should monitor the training loss. The objective is to minimize this loss. Monitoring allows the callback to track the performance of the model and make decisions based on it.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">patience=5</span></code></strong>: Patience determines the number of epochs with no improvement after which training will be stopped. In this case, training will continue for 5 more epochs without any improvement in the monitored metric (loss) before stopping.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">min_delta=0</span></code></strong>: This parameter sets the minimum change in the monitored quantity to qualify as an improvement. Setting it to 0 means any improvement in loss, no matter how small, is considered significant as long as it is a decrease.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">mode='min'</span></code></strong>: Since the goal is to minimize the loss, the mode is set to ‘min’. This means training will stop when the quantity monitored (loss) has stopped decreasing. If the mode were ‘max’, it would stop training when the monitored quantity stops increasing (useful for metrics like accuracy).</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">verbose=1</span></code></strong>: This enables verbose output, which means the epoch at which training stops will be printed to the console.</p></li>
</ul>
</section>
<section id="lstm-model-creation">
<h3>LSTM Model Creation<a class="headerlink" href="#lstm-model-creation" title="Link to this heading">#</a></h3>
<p>Following the early stopping configuration, the code initializes the LSTM model using a <code class="docutils literal notranslate"><span class="pre">create_lstm_model</span></code> function, which is not fully detailed here but likely constructs a sequential model with LSTM layers. The parameters <code class="docutils literal notranslate"><span class="pre">vocab_size</span></code> and <code class="docutils literal notranslate"><span class="pre">max_seq_len</span></code> suggest the model is designed for text processing, where <code class="docutils literal notranslate"><span class="pre">vocab_size</span></code> indicates the size of the vocabulary (number of unique words) the model can recognize, and <code class="docutils literal notranslate"><span class="pre">max_seq_len</span></code> specifies the length of the input sequences.</p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">batch_size=32</span></code></strong>: This specifies that 32 samples will be processed before the model’s internal parameters are updated. Batch size is an important hyperparameter that can affect the convergence and performance of the model.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Add early stopping to prevent overfitting.</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.callbacks</span> <span class="kn">import</span> <span class="n">EarlyStopping</span>
<span class="n">early_stopping_callback</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span>
    <span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span>  <span class="c1"># Monitor the training loss</span>
    <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># Number of epochs with no improvement after which training will be stopped</span>
    <span class="n">min_delta</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># Minimum change in the monitored quantity to qualify as an improvement</span>
    <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span>  <span class="c1"># In &#39;min&#39; mode, training will stop when the quantity monitored has stopped decreasing</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>

<span class="c1"># Create the LSTM model</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">create_lstm_model</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;LSTM&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_2 (Embedding)     (None, 162, 150)          4108500   
                                                                 
 lstm_3 (LSTM)               (None, 150)               180600    
                                                                 
 dense_2 (Dense)             (None, 27390)             4135890   
                                                                 
=================================================================
Total params: 8424990 (32.14 MB)
Trainable params: 8424990 (32.14 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Link to this heading">#</a></h2>
<p>We finally run the training of our LSTM model with the setup previously discussed, including the early stopping mechanism to prevent overfitting. This process involves feeding the model with our preprocessed and padded sequences of text data, along with their corresponding targets, and allowing it to learn how to predict the next word in a sequence based on its current understanding of the text’s structure and content.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">from</span> <span class="nn">keras.callbacks</span> <span class="kn">import</span> <span class="n">TensorBoard</span>

<span class="c1"># Create a unique name for the log directory based on the current time and set callback</span>
<span class="n">log_dir</span> <span class="o">=</span> <span class="s2">&quot;logs/fit/&quot;</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="s2">&quot;batch&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="o">+</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;%Y%m</span><span class="si">%d</span><span class="s2">-%H%M%S&quot;</span><span class="p">)</span>
<span class="n">tensorboard_callback</span> <span class="o">=</span> <span class="n">TensorBoard</span><span class="p">(</span><span class="n">log_dir</span><span class="o">=</span><span class="n">log_dir</span><span class="p">,</span> <span class="n">histogram_freq</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">input_sequences</span><span class="p">,</span>
          <span class="n">target_words</span><span class="p">,</span>
          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
          <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">tensorboard_callback</span><span class="p">,</span> <span class="n">early_stopping_callback</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/500
5491/5491 [==============================] - 109s 19ms/step - loss: 6.9633 - accuracy: 0.0431
Epoch 2/500
5491/5491 [==============================] - 90s 16ms/step - loss: 6.3297 - accuracy: 0.0803
Epoch 3/500
5491/5491 [==============================] - 88s 16ms/step - loss: 6.0070 - accuracy: 0.0977
Epoch 4/500
5491/5491 [==============================] - 87s 16ms/step - loss: 5.7569 - accuracy: 0.1085
Epoch 5/500
5491/5491 [==============================] - 87s 16ms/step - loss: 5.5293 - accuracy: 0.1175
Epoch 6/500
...
...
5491/5491 [==============================] - 86s 16ms/step - loss: 1.1493 - accuracy: 0.7512
Epoch 135/500
5491/5491 [==============================] - 86s 16ms/step - loss: 1.1521 - accuracy: 0.7501
Epoch 136/500
5491/5491 [==============================] - 87s 16ms/step - loss: 1.1520 - accuracy: 0.7494
Epoch 137/500
5491/5491 [==============================] - 86s 16ms/step - loss: 1.2544 - accuracy: 0.7220
Epoch 138/500
5491/5491 [==============================] - 86s 16ms/step - loss: 1.3315 - accuracy: 0.6990
Epoch 139/500
5491/5491 [==============================] - 87s 16ms/step - loss: 1.1972 - accuracy: 0.7371
Epoch 139: early stopping
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;keras.src.callbacks.History at 0x788ae09b1900&gt;
</pre></div>
</div>
</div>
</div>
</section>
<section id="plotting-the-training-history-and-saving-the-model">
<h2>Plotting the training history and saving the model<a class="headerlink" href="#plotting-the-training-history-and-saving-the-model" title="Link to this heading">#</a></h2>
<p>Let us now plot the training history, and save the model. We are saving the trained model on the Google Drive to ensure it doesn’t get deleted once the running session ends.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the training history</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">plot_training_history</span><span class="p">(</span><span class="n">history</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Model accuracy and loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy and Loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;Loss&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plot_training_history</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">history</span><span class="p">)</span>

<span class="c1"># Save the model</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;/content/drive/MyDrive/Colab Notebooks/shakespeare-&quot;</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;-embeddings.keras&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2e7be1670b7fb887b680a2ffeaf2d87c0d9070ab41273bf3dc9a8af081f8cbbb.png" src="../_images/2e7be1670b7fb887b680a2ffeaf2d87c0d9070ab41273bf3dc9a8af081f8cbbb.png" />
</div>
</div>
</section>
<section id="perform-a-prediction-or-two">
<h2>Perform a prediction or two<a class="headerlink" href="#perform-a-prediction-or-two" title="Link to this heading">#</a></h2>
<p>Finally we can run predictions based on seed inputs, to see how the model performs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the model, and the tokenizer</span>

<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>
<span class="kn">import</span> <span class="nn">pickle</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;/content/drive/MyDrive/Colab Notebooks/shakespeare-&quot;</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;-embeddings.keras&quot;</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;/content/drive/MyDrive/Colab Notebooks/shakespeare-tokenizer.pickle&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">handle</span><span class="p">:</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>

<span class="c1"># Predict next words based on an input sequence</span>

<span class="k">def</span> <span class="nf">predict_next_words</span><span class="p">(</span><span class="n">seed_text</span><span class="p">,</span> <span class="n">next_words</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">next_words</span><span class="p">):</span>
      <span class="n">token_list</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">([</span><span class="n">seed_text</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">token_list</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">([</span><span class="n">token_list</span><span class="p">],</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">max_seq_len</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;pre&#39;</span><span class="p">)</span>
      <span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">token_list</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">predicted_class</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predicted</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">output_word</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
      <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
          <span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="n">predicted_class</span><span class="p">:</span>
              <span class="n">output_word</span> <span class="o">=</span> <span class="n">word</span>
              <span class="k">break</span>
      <span class="n">seed_text</span> <span class="o">+=</span> <span class="s2">&quot; &quot;</span> <span class="o">+</span> <span class="n">output_word</span>
  <span class="k">return</span> <span class="n">seed_text</span>

<span class="n">seed_texts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;My kingdom for&quot;</span><span class="p">,</span> <span class="s2">&quot;My daughter is&quot;</span><span class="p">,</span> <span class="s2">&quot;If only I were&quot;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">seed_text</span> <span class="ow">in</span> <span class="n">seed_texts</span><span class="p">:</span>
  <span class="n">next_words</span> <span class="o">=</span> <span class="mi">15</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">predict_next_words</span><span class="p">(</span><span class="n">seed_text</span><span class="p">,</span> <span class="n">next_words</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>My kingdom for you are old and say not be true love you fortunes of good duke of
My daughter is to know you and my sister nothing my lord friend being nothing now of me
If only I were a soldier he is differency between a silver and it from the king henry man
</pre></div>
</div>
</div>
</div>
</section>
<section id="final-remarks">
<h2>Final remarks<a class="headerlink" href="#final-remarks" title="Link to this heading">#</a></h2>
<p>We could demonstrate very simple, contextual text generation based on a primitive Recurrent Neural Network (RNN). Despite being far from the capabilities of a large language model, we can still illustrate the foundational principles that enable these models to generate text based on learned patterns from a given dataset. Through this simplified example, we aim to shed light on how RNNs can grasp basic language structures and produce text that, while not as sophisticated or coherent as that generated by more advanced models like GPT or BERT, still reflects the initial steps towards understanding and replicating human language.</p>
<p>Our basic RNN, although limited, can be trained on a small dataset of text to learn sequences of words or characters. After training, it can generate new sequences of text by predicting the next word or character based on a given seed text. This process starts with the model receiving an initial input (the seed), processing it to make a prediction, and then feeding the prediction back into the model as part of the next input sequence.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./experiments"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="sentiment-partI.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Sentiment Classification of Financial Texts using Neural Networks</p>
      </div>
    </a>
    <a class="right-next"
       href="gensim.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Basics of word vectors</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">An Exploration of Language Prediction: A Simplified Model for Shakespearean Text Generation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mount-google-drive-into-colab-and-read-our-dataset">Mount Google Drive into Colab and read our dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#text-tokenization">Text Tokenization</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#a-function-to-create-n-grams">A function to create n-grams</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-example-n-gram-output">An example n-gram output</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-the-model">Defining the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#padding-sequences">Padding sequences</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#verifying-sequences">Verifying sequences</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#early-stopping-and-instantiating-the-model">Early stopping and instantiating the model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#early-stopping">Early Stopping</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lstm-model-creation">LSTM Model Creation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plotting-the-training-history-and-saving-the-model">Plotting the training history and saving the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#perform-a-prediction-or-two">Perform a prediction or two</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-remarks">Final remarks</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Pedro Leitao
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <div>
<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png"></a>
    All content on this site (unless otherwise specified) is licensed under the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0 license</a>
</div>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>