
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Sentiment Classification of Financial Texts using Neural Networks &#8212; Pedro&#39;s experiments</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'experiments/sentiment-partI';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="An Exploration of Language Prediction: A Simplified Model for Shakespearean Text Generation" href="shakespeare-embeddings.html" />
    <link rel="prev" title="A Wine Quality Prediction Experiment with SKLearn Pipelines" href="ml-pipeline-sklearn.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo-new.png" class="logo__image only-light" alt="Pedro's experiments - Home"/>
    <script>document.write(`<img src="../_static/logo-new.png" class="logo__image only-dark" alt="Pedro's experiments - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Pedro’s experiments
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">How to's</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../howtos/apple-silicon-ml.html">Setting up a Machine Learning environment on Apple Silicon</a></li>
<li class="toctree-l1"><a class="reference internal" href="../howtos/distributed-training.html">Distributed training with Keras &amp; Tensorflow</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Experiments</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="instance_vs_model_learning.html">Instance vs Model Learning</a></li>


<li class="toctree-l1"><a class="reference internal" href="ml-pipeline-sklearn.html">A Wine Quality Prediction Experiment with SKLearn Pipelines</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Sentiment Classification of Financial Texts using Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="shakespeare-embeddings.html">An Exploration of Language Prediction: A Simplified Model for Shakespearean Text Generation</a></li>

<li class="toctree-l1"><a class="reference internal" href="gensim.html">Basics of word vectors</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/pedro-leitao/pedro-leitao.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/experiments/sentiment-partI.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Sentiment Classification of Financial Texts using Neural Networks</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-processing">Pre-processing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#importing-the-data">Importing the data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenizing-content">Tokenizing content</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#splitting-the-dataset-into-training-and-validation">Splitting the dataset into training and validation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-the-classification-model">Defining the classification model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-model-layers">Understanding the model layers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-compilation">Model Compilation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-the-model">Training the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plotting-and-visualizing-the-training-history">Plotting and visualizing the training history</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualising-embeddings">Visualising embeddings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-remarks">Final remarks</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="sentiment-classification-of-financial-texts-using-neural-networks">
<h1>Sentiment Classification of Financial Texts using Neural Networks<a class="headerlink" href="#sentiment-classification-of-financial-texts-using-neural-networks" title="Link to this heading">#</a></h1>
<p>Sentimentß classification stands as a pivotal task within the field of natural language processing (NLP), offering significant utility across various applications. This experiment focuses on crafting a straightforward neural network model aimed at categorizing sentences into predefined sentiment categories: positive, neutral, or negative.</p>
<p>The task at hand involves implementing a basic neural network architecture for the purpose of sentiment classification. This model endeavors to systematically assign financial sentences to one of the three sentiment classes mentioned. The dataset employed for training and evaluation originates from <a class="reference external" href="https://www.kaggle.com/datasets/sbhatti/financial-sentiment-analysis">Kaggle</a> and encompasses approximately 5,000 financial sentences. This dataset serves as the foundation for our experiment, providing a small(ish) range of sentences for model training and validation.</p>
<p>Our objective is to explore the efficacy of simple network structures in accurately classifying sentiment within the domain of financial texts. Through this exercise, we aim to gain insights into the challenges and potential of using neural networks for sentiment analysis in financial contexts.</p>
<p>We start by loading the data into separate sentences and sentiments. The original data is a simple CSV in the form:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sentence</span> <span class="n">A</span><span class="p">,</span><span class="n">sentiment</span> <span class="n">A</span>
<span class="n">sentence</span> <span class="n">B</span><span class="p">,</span><span class="n">sentiment</span> <span class="n">B</span>
<span class="o">...</span>
</pre></div>
</div>
<section id="pre-processing">
<h2>Pre-processing<a class="headerlink" href="#pre-processing" title="Link to this heading">#</a></h2>
<p>In the pre-processing phase, we will simply load our CSV dataset into a list of entries, composed of sentences and sentiments.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>

<span class="k">def</span> <span class="nf">read_csv_data</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
    <span class="c1"># Create an empty list to store the data</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Open the CSV file</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="c1"># Create a CSV reader object</span>
        <span class="n">reader</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">reader</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
        
        <span class="c1"># Skip the header row if it exists</span>
        <span class="nb">next</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>
        
        <span class="c1"># Iterate over each row in the CSV file</span>
        <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">reader</span><span class="p">:</span>
            <span class="c1"># Extract the sentence and sentiment from the row</span>
            <span class="n">sentence</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">sentiment</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            
            <span class="c1"># Create a dictionary to store the sentence and sentiment</span>
            <span class="n">entry</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;sentence&#39;</span><span class="p">:</span> <span class="n">sentence</span><span class="p">,</span> <span class="s1">&#39;sentiment&#39;</span><span class="p">:</span> <span class="n">sentiment</span><span class="p">}</span>
            
            <span class="c1"># Append the entry to the data list</span>
            <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">entry</span><span class="p">)</span>

    <span class="c1"># Return the data list</span>
    <span class="k">return</span> <span class="n">data</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="importing-the-data">
<h2>Importing the data<a class="headerlink" href="#importing-the-data" title="Link to this heading">#</a></h2>
<p>At this stage, we’re prepared to load the data into appropriate data structures. It’s important to note the conversion of sentiment labels from strings to numerical values, ranging from 0 (negative) to 2 (positive). Additionally, it’s necessary to explicitly convert the list of sentiments into a NumPy array for further processing.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import data</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">read_csv_data</span><span class="p">(</span><span class="s1">&#39;.data/data.csv&#39;</span><span class="p">)</span>

<span class="c1"># Prepare the dataset</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span><span class="p">[</span><span class="s1">&#39;sentence&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
<span class="n">sentiments</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span> <span class="k">if</span> <span class="n">d</span><span class="p">[</span><span class="s1">&#39;sentiment&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;positive&#39;</span> <span class="k">else</span> <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="n">d</span><span class="p">[</span><span class="s1">&#39;sentiment&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;neutral&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
<span class="c1"># Convert the sentiments list to a NumPy array as this is the format Keras requires</span>
<span class="n">sentiments</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">sentiments</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">sentences</span></code> is just a list (as a Numpy array) as such:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&quot;The GeoSolutions technology will leverage Benefon &#39;s GPS solutions by providing Location Based Search Technology , a Communities Platform , location relevant multimedia content and a new and powerful commercial model .&quot;, &#39;$ESI on lows, down $1.50 to $2.50 BK a real possibility&#39;]
</pre></div>
</div>
</div>
</div>
<p>In our case, the sentiments variable, which we convert into a NumPy array, comprises a list of integers where the value 2 represents positive, 1 denotes neutral, and 0 signifies negative sentiment. This numerical encoding, utilizing only non-negative integers, is essential for compatibility with the <code class="docutils literal notranslate"><span class="pre">sparse_categorical_crossentropy</span></code> loss function. This particular loss function mandates the use of non-negative integers, as it relies on these values to interpret the class labels correctly for model training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">sentiments</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[2 0]
</pre></div>
</div>
</div>
</div>
</section>
<section id="tokenizing-content">
<h2>Tokenizing content<a class="headerlink" href="#tokenizing-content" title="Link to this heading">#</a></h2>
<p>In our preprocessing steps, we will tokenize the input sentences, constraining them to a vocabulary size of 5000 words. This limitation is intentionally set to foster a compact word space, which is important to aid the neural network’s ability to generalize more effectively across unseen data. A smaller vocabulary size is an important choice to mitigate the risk of overfitting, as it encourages the model to focus on the most relevant features that are truly indicative of the sentiments expressed in the sentences, rather than memorizing specific, less frequent terms that may not contribute to a robust understanding of the text.</p>
<p>We also set a “Out of Vocabulary” token, meaning that the model will replace any words not in the vocabulary with this token. This is a crucial step to ensure that the model can handle unseen words during inference, preventing it from becoming confused or making incorrect predictions when encountering previously unseen terms. We will also filter out any punctuation marks and other characters from the sentences, as they are not typically indicative of sentiment and could introduce noise into the model’s learning process.</p>
<p>Additionally, we will ensure that all tokenized sequences are padded to a uniform length of 50 tokens. This standardization is crucial for maintaining consistent input dimensions required by neural networks, facilitating batch processing and ensuring that the model can efficiently learn from sequences of a standardized shape. The choice of 50 tokens as the maximum sequence length is a balance between retaining sufficient contextual information and maintaining computational efficiency, preventing the model from becoming bogged down by excessively long inputs that could dilute meaningful signals with noise.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>

<span class="c1"># Tokenize the dataset</span>
<span class="n">max_words</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">max_len</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">max_words</span><span class="p">,</span> <span class="n">oov_token</span><span class="o">=</span><span class="s1">&#39;&lt;OOV&gt;&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="s1">&#39;!&quot;#&amp;()*+,-./:;&lt;=&gt;?@[</span><span class="se">\\</span><span class="s1">]</span><span class="se">\&#39;</span><span class="s1">^_`{|}~</span><span class="se">\t\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>
<span class="n">padded_sequences</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">truncating</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">max_len</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And let us look at what the tokenized sentences look like.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print the first sentence and its padded sequence</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">padded_sequences</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">sequences_to_texts</span><span class="p">([</span><span class="n">padded_sequences</span><span class="p">[</span><span class="mi">0</span><span class="p">]]))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">padded_sequences</span><span class="p">),</span> <span class="n">padded_sequences</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">sentiments</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentiments</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The GeoSolutions technology will leverage Benefon &#39;s GPS solutions by providing Location Based Search Technology , a Communities Platform , location relevant multimedia content and a new and powerful commercial model .
[   2 3968  135   14 3127 1407   10 2277  134   19  823 1221  101 2278
  135    7 3128 1222 1221 2010 3129  682    6    7   59    6 3969  581
  473    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0]
[&#39;the geosolutions technology will leverage benefon s gps solutions by providing location based search technology a communities platform location relevant multimedia content and a new and powerful commercial model &lt;OOV&gt; &lt;OOV&gt; &lt;OOV&gt; &lt;OOV&gt; &lt;OOV&gt; &lt;OOV&gt; &lt;OOV&gt; &lt;OOV&gt; &lt;OOV&gt; &lt;OOV&gt; &lt;OOV&gt; &lt;OOV&gt; &lt;OOV&gt; &lt;OOV&gt; &lt;OOV&gt; &lt;OOV&gt; &lt;OOV&gt; &lt;OOV&gt; &lt;OOV&gt; &lt;OOV&gt; &lt;OOV&gt;&#39;]
&lt;class &#39;numpy.ndarray&#39;&gt; (5842, 50)
&lt;class &#39;numpy.ndarray&#39;&gt; 5842
</pre></div>
</div>
</div>
</div>
</section>
<section id="splitting-the-dataset-into-training-and-validation">
<h2>Splitting the dataset into training and validation<a class="headerlink" href="#splitting-the-dataset-into-training-and-validation" title="Link to this heading">#</a></h2>
<p>Splitting a dataset into training and validation subsets is a standard practice in machine learning and deep learning projects. This approach allows for the evaluation of model performance on unseen data, providing insights into how well the model is likely to perform on real-world data or under general conditions.</p>
<p>In our specific scenario, we have decided to allocate 20% of our dataset for validation purposes. This means that 80% of the data will be utilized for training the model, enabling it to learn and adapt to the patterns present in the data. The remaining 20% will serve as the validation set, which will not be used during the training process. Instead, it will be used to evaluate the model’s performance, helping us to monitor and fine-tune the model’s parameters and architecture to prevent overfitting and underfitting, ensuring that the model generalizes well to new, unseen data.</p>
<p>By employing this split, we can achieve a balance between having enough data for the model to learn effectively and retaining a sufficient amount of data to reliably assess its performance. This strategy is essential for developing robust models that perform well in practical applications.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Split the dataset into training and validation sets</span>
<span class="n">training_sentences</span><span class="p">,</span> <span class="n">validation_sentences</span><span class="p">,</span> <span class="n">training_sentiments</span><span class="p">,</span> <span class="n">validation_sentiments</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">padded_sequences</span><span class="p">,</span> <span class="n">sentiments</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training dataset:&quot;</span><span class="p">,</span> <span class="s2">&quot;sentences -&quot;</span><span class="p">,</span> <span class="n">training_sentences</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;sentiments -&quot;</span><span class="p">,</span> <span class="n">training_sentiments</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Validation dataset:&quot;</span><span class="p">,</span> <span class="s2">&quot;sentences -&quot;</span><span class="p">,</span> <span class="n">validation_sentences</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;sentiments -&quot;</span><span class="p">,</span> <span class="n">validation_sentiments</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training dataset: sentences - (4673, 50) sentiments - (4673,)
Validation dataset: sentences - (1169, 50) sentiments - (1169,)
</pre></div>
</div>
</div>
</div>
</section>
<section id="defining-the-classification-model">
<h2>Defining the classification model<a class="headerlink" href="#defining-the-classification-model" title="Link to this heading">#</a></h2>
<p>We can now go about building the model for classification. We separate model parameters into <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> and <code class="docutils literal notranslate"><span class="pre">embedding_dim</span></code> - this allows us to perform multiple comparative runs with different model parameters, which are then logged under TensorBoard for evaluation.</p>
<p>Assuming you have installed TensorBoard with <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">tensorboard</span></code>, you can visualise results by running the command <code class="docutils literal notranslate"><span class="pre">tensorboard</span> <span class="pre">--logdir</span> <span class="pre">logs/model-nn</span></code> and loading <code class="docutils literal notranslate"><span class="pre">https://localhost:6006</span></code>.</p>
<p>We also configure early stopping of the training once loss and training accuracy level off.</p>
<div class="alert alert-block alert-info">
<b>Note: </b>
Batch size can have a significant impact on the training process.
<p>Smaller batch sizes:</p>
<ul class="simple">
<li><p>Faster Convergence: Smaller batches often lead to faster convergence because the model’s weights are updated more frequently. However, these updates are noisier, which can be beneficial as it might help the model escape local minima.</p></li>
<li><p>Regularization Effect: The noise introduced by smaller batches can also serve as a form of regularization, potentially improving generalization and thus accuracy on unseen data.</p></li>
<li><p>Memory Efficiency: Smaller batches are more memory-efficient, enabling training on limited resources without compromising the ability to model complex relationships.</p></li>
<li><p>Potential for Overfitting: While the regularization effect can improve generalization, there’s also a risk of overfitting if the batch size is too small, especially if not complemented with proper regularization techniques.</p></li>
</ul>
<p>Larger batch sizes:</p>
<ul class="simple">
<li><p>Stable Gradient Estimates: Larger batches provide more stable estimates of the gradient, which can lead to smoother convergence. However, this stability may also lead the model to settle in sharp minima, which could generalize poorly.</p></li>
<li><p>Reduced Regularization Effect: The reduced noise in the gradient estimates diminishes the implicit regularization effect seen with smaller batches, potentially necessitating explicit regularization methods.</p></li>
<li><p>Higher Memory Requirements: Larger batch sizes require more memory, which can limit model complexity or batch size on hardware with limited resources.</p></li>
<li><p>Efficiency and Speed: Training with larger batches can be more computationally efficient due to better utilization of hardware resources, such as GPUs, potentially reducing the time per epoch.</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Build a classification model using a neural network</span>

<span class="c1"># Define identifiable information</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;model-nn&#39;</span>
<span class="n">timestamp</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;%Y%m</span><span class="si">%d</span><span class="s2">-%H%M%S&quot;</span><span class="p">)</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># Create the model</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">GlobalAveragePooling1D</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
    <span class="n">Embedding</span><span class="p">(</span><span class="n">max_words</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">max_len</span><span class="p">,)),</span>
    <span class="n">GlobalAveragePooling1D</span><span class="p">(),</span>
    <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
    <span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># Define a lower learning rate</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="c1"># Create an Adam optimizer with the desired learning rate</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="c1"># Compile the model</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;sparse_categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/pedroleitao/.pyenv/versions/3.9.18/envs/tf/lib/python3.9/site-packages/keras/src/layers/core/embedding.py:81: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
2024-04-08 09:57:26.461609: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2
2024-04-08 09:57:26.461626: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 24.00 GB
2024-04-08 09:57:26.461631: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 8.00 GB
2024-04-08 09:57:26.461665: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2024-04-08 09:57:26.461678: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -&gt; physical PluggableDevice (device: 0, name: METAL, pci bus id: &lt;undefined&gt;)
</pre></div>
</div>
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential"</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ embedding (<span style="color: #0087ff; text-decoration-color: #0087ff">Embedding</span>)           │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">50</span>, <span style="color: #00af00; text-decoration-color: #00af00">4</span>)          │        <span style="color: #00af00; text-decoration-color: #00af00">20,000</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ global_average_pooling1d        │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">4</span>)              │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">GlobalAveragePooling1D</span>)        │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">4</span>)              │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                   │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">3</span>)              │            <span style="color: #00af00; text-decoration-color: #00af00">15</span> │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">20,015</span> (78.18 KB)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">20,015</span> (78.18 KB)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div></div>
</div>
</section>
<section id="understanding-the-model-layers">
<h2>Understanding the model layers<a class="headerlink" href="#understanding-the-model-layers" title="Link to this heading">#</a></h2>
<p>This model is a simple neural network designed for text processing, utilizing an embedding layer at the beginning followed by dropout, pooling, and dense layers. Let’s break down each layer and its purpose:</p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Embedding</span></code> Layer</strong>: This is the first layer of the model, which turns positive integers (indexes) into dense vectors of fixed size, <code class="docutils literal notranslate"><span class="pre">embedding_dim</span></code>. The <code class="docutils literal notranslate"><span class="pre">max_words</span></code> parameter defines the size of the vocabulary in the text data, and <code class="docutils literal notranslate"><span class="pre">max_len</span></code> specifies the length of input sequences. The embedding layer is crucial for text processing as it learns to represent words in a high-dimensional space in a way that captures semantic relationships between them.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">GlobalAveragePooling1D</span></code> Layer</strong>: This layer reduces the dimensionality of the input by computing the mean of all the dimensions for each sample. It’s useful for reducing the computational cost and simplifying the model while retaining important information. In the context of text, it helps to summarize the essential features of the embeddings across the sequence.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Dropout</span></code> Layer</strong>: This layer randomly sets input units to 0 with a frequency of 0.2 at each step during training time, which helps prevent overfitting. It’s a form of regularization that forces the model to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Dense</span></code> Layer</strong>: This is the output layer of the model, with 3 units and a softmax activation function, indicating that the model is designed for a classification task with 3 classes. The softmax function ensures that the output values are in the range (0, 1) and sum up to 1, making them interpretable as class probabilities.</p></li>
</ul>
<section id="model-compilation">
<h3>Model Compilation<a class="headerlink" href="#model-compilation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The model is compiled with the <code class="docutils literal notranslate"><span class="pre">adam</span></code> optimizer, which is an adaptive learning rate optimization algorithm designed for training deep neural networks.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">sparse_categorical_crossentropy</span></code> loss function is used, suitable for multi-class classification tasks where the labels are integers (a common scenario in classification problems).</p></li>
<li><p>The metric used to evaluate the model is accuracy, which is typical for classification tasks.</p></li>
</ul>
</section>
</section>
<section id="training-the-model">
<h2>Training the model<a class="headerlink" href="#training-the-model" title="Link to this heading">#</a></h2>
<p>Finally we run and train the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a log directory with the identifiable information</span>
<span class="n">log_dir</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;logs/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">/batchsize</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">timestamp</span><span class="si">}</span><span class="s2">_maxwords</span><span class="si">{</span><span class="n">max_words</span><span class="si">}</span><span class="s2">_embedding</span><span class="si">{</span><span class="n">embedding_dim</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="c1"># Set up the TensorBoard callback</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.callbacks</span> <span class="kn">import</span> <span class="n">TensorBoard</span>

<span class="n">tensorboard_callback</span> <span class="o">=</span> <span class="n">TensorBoard</span><span class="p">(</span><span class="n">log_dir</span><span class="o">=</span><span class="n">log_dir</span><span class="p">,</span> <span class="n">histogram_freq</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">tensorflow.keras.callbacks</span> <span class="kn">import</span> <span class="n">EarlyStopping</span>

<span class="n">early_stopping_callback</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">restore_best_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Train the model</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">training_sentences</span><span class="p">,</span> <span class="n">training_sentiments</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">validation_sentences</span><span class="p">,</span> <span class="n">validation_sentiments</span><span class="p">),</span>
                    <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="c1"># Set to 1 to print the training log</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">tensorboard_callback</span><span class="p">,</span><span class="n">early_stopping_callback</span><span class="p">])</span>

<span class="c1"># Print training statistics</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training accuracy:&quot;</span><span class="p">,</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Validation accuracy:&quot;</span><span class="p">,</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_accuracy&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-04-08 09:57:26.738601: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training accuracy: 0.8367215991020203
Validation accuracy: 0.7117193937301636
</pre></div>
</div>
</div>
</div>
</section>
<section id="plotting-and-visualizing-the-training-history">
<h2>Plotting and visualizing the training history<a class="headerlink" href="#plotting-and-visualizing-the-training-history" title="Link to this heading">#</a></h2>
<p>At this stage, we can visualize the progression of our model’s training through the lenses of accuracy and loss metrics. These two indicators provide essential insights into how well our model is performing and learning over time.</p>
<p><strong>Accuracy</strong> is a measure of how often the model’s predictions match the true labels of the data. It’s a direct indicator of the model’s performance, with higher values indicating better performance. In the context of training and validation, we typically observe two accuracy trends: one for the training set (showing how well the model fits the data it learns from) and one for the validation set (indicating how well the model generalizes to new, unseen data). An ideal model demonstrates high accuracy on both training and validation sets, suggesting it has learned well and can generalize its learning effectively.</p>
<p><strong>Loss</strong>, on the other hand, quantifies the difference between the model’s predicted values and the actual values for a given number of instances. It is a critical measure used during the training process to adjust the model’s weights with the objective of minimizing this difference, hence improving the model’s predictions. Like accuracy, we monitor loss for both training and validation datasets. A decreasing trend in loss over epochs signifies that the model is learning correctly. However, if validation loss starts to increase while training loss continues to decrease, it could indicate overfitting—meaning the model is performing well on the training data but poorly on new, unseen data.</p>
<p>Visualizing accuracy and loss during training provides valuable feedback on the learning process. It helps identify when the model has started to overfit, underfit, or if it’s learning as expected. These insights guide adjustments to the model architecture, the training process, or even to the data itself, with the goal of achieving a well-performing model that generalizes well to new data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;training accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_accuracy&#39;</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;validation accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training and validation accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;training loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;validation loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training and validation loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/d9ca6256513387d9d65a5bac4d9fb86e2c4da3bb482593cf9424e530df2b5915.png" src="../_images/d9ca6256513387d9d65a5bac4d9fb86e2c4da3bb482593cf9424e530df2b5915.png" />
<img alt="../_images/3c737aefe05f4d53bcef5f64977c244185a7cdb51dc87eaa12b0fe9aee1cd22c.png" src="../_images/3c737aefe05f4d53bcef5f64977c244185a7cdb51dc87eaa12b0fe9aee1cd22c.png" />
</div>
</div>
<p>We can now try out some predictions on the model to see how it performs on unseen data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Try a few predictions</span>

<span class="c1"># Define a function to predict the sentiment of a sentence</span>
<span class="k">def</span> <span class="nf">predict_sentiment</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>
    <span class="c1"># Tokenize the sentence</span>
    <span class="n">sequence</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">([</span><span class="n">sentence</span><span class="p">])</span>
    
    <span class="c1"># Pad the sequence</span>
    <span class="n">padded_sequence</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">truncating</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">max_len</span><span class="p">)</span>
    
    <span class="c1"># Get the prediction</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">padded_sequence</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># Get the sentiment with the highest probability</span>
    <span class="n">sentiment</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">sentiment</span><span class="p">,</span> <span class="n">prediction</span>

<span class="c1"># Predict the sentiment of a positive sentence</span>
<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;IBM reported an increase in revenue during the last quarter&quot;</span>
<span class="n">sentiment</span><span class="p">,</span> <span class="n">predictions</span> <span class="o">=</span> <span class="n">predict_sentiment</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sentence: </span><span class="si">{</span><span class="n">sentence</span><span class="si">}</span><span class="s2">, Predictions: </span><span class="si">{</span><span class="n">predictions</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># Print the sentiment</span>
<span class="k">if</span> <span class="n">sentiment</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Correctly predicted - Sentiment: Positive&quot;</span><span class="p">)</span>

<span class="c1"># Predict the sentiment of a neutral sentence</span>
<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;IBM is a technology company&quot;</span>
<span class="n">sentiment</span><span class="p">,</span> <span class="n">predictions</span> <span class="o">=</span> <span class="n">predict_sentiment</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sentence: </span><span class="si">{</span><span class="n">sentence</span><span class="si">}</span><span class="s2">, Predictions: </span><span class="si">{</span><span class="n">predictions</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">if</span> <span class="n">sentiment</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Correctly predicted - Sentiment: Neutral&quot;</span><span class="p">)</span>

<span class="c1"># Predict the sentiment of a negative sentence</span>
<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;IBM down $3.4 in yesterday&#39;s trading session&quot;</span>
<span class="n">sentiment</span><span class="p">,</span> <span class="n">predictions</span> <span class="o">=</span> <span class="n">predict_sentiment</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sentence: </span><span class="si">{</span><span class="n">sentence</span><span class="si">}</span><span class="s2">, Predictions: </span><span class="si">{</span><span class="n">predictions</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">if</span> <span class="n">sentiment</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Correctly predicted - Sentiment: Negative&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">1/1</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 48ms/step
Sentence: IBM reported an increase in revenue during the last quarter, Predictions: [0.08229597 0.24526857 0.6724355 ]
Correctly predicted - Sentiment: Positive
<span class=" -Color -Color-Bold">1/1</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 9ms/step
Sentence: IBM is a technology company, Predictions: [0.10290223 0.7102392  0.18685855]
Correctly predicted - Sentiment: Neutral
<span class=" -Color -Color-Bold">1/1</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 9ms/step
Sentence: IBM down $3.4 in yesterday&#39;s trading session, Predictions: [0.347476  0.3561046 0.2964194]
</pre></div>
</div>
</div>
</div>
<p>In the above output, we can see the model’s predictions for a few sample sentences. The model assigns a sentiment label to each sentence, with the corresponding probabilities for each class. The sentiment label with the highest probability is considered the model’s prediction for that sentence. Keep in mind the index mapping for sentiment labels: 0 for negative, 1 for neutral, and 2 for positive. In the first sentence example, the model predicts <code class="docutils literal notranslate"><span class="pre">[0.10788833</span> <span class="pre">0.1402162</span>&#160; <span class="pre">0.7518954]</span></code>, or an 77% probability for a positive sentiment.</p>
<div class="alert alert-block alert-warning">
<b>Warning: </b>
An important aspect of developing any model, is understanding your training data and the context in which the model will be used. In this case, the financial sentiment dataset is relatively small and will not be representative enough. You should also consider the potential biases in the data and how they might impact the model's predictions, for example, in our dataset we have an uneven distribution of sentiment classes:
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>grep<span class="w"> </span><span class="s2">&quot;,negative&quot;</span><span class="w"> </span><span class="nv">$HOME</span>/Downloads/data.csv<span class="p">|</span>wc<span class="w"> </span>-l
<span class="w">     </span><span class="m">860</span>
grep<span class="w"> </span><span class="s2">&quot;,neutral&quot;</span><span class="w"> </span><span class="nv">$HOME</span>/Downloads/data.csv<span class="p">|</span>wc<span class="w"> </span>-l
<span class="w">    </span><span class="m">3130</span>
grep<span class="w"> </span><span class="s2">&quot;,positive&quot;</span><span class="w"> </span><span class="nv">$HOME</span>/Downloads/data.csv<span class="p">|</span>wc<span class="w"> </span>-l
<span class="w">    </span><span class="m">1852</span>
</pre></div>
</div>
<p>It’s important to keep these limitations in mind when interpreting the model’s predictions and deciding on its deployment.</p>
</div></section>
<section id="visualising-embeddings">
<h2>Visualising embeddings<a class="headerlink" href="#visualising-embeddings" title="Link to this heading">#</a></h2>
<p>Additionally, visualizing the output of the embeddings layer offers a comprehensive understanding of how the model represents words within the embedding space. This visualization sheds light on the model’s ability to capture semantic relationships among words, while also illustrating how these relationships are utilized to enhance prediction accuracy. In this context, we employ a K-means clustering algorithm to categorize similar words into clusters within the embedding space. This technique clarifies the model’s internal representations, and aids in identifying patterns and insights that could potentially improve model performance by revealing the underlying structure of the embedding space.</p>
<div class="alert alert-block alert-info">
<img src="https://upload.wikimedia.org/wikipedia/commons/9/94/T-SNE_visualisation_of_word_embeddings_generated_using_19th_century_literature.png" width=250 align=right>
<b>Note: </b>
t-SNE, short for t-Distributed Stochastic Neighbor Embedding, is a machine learning algorithm used for dimensionality reduction, particularly well-suited for visualizing high-dimensional datasets. It works by converting similarities between data points to joint probabilities and tries to minimize the difference between these joint probabilities in the high-dimensional and low-dimensional spaces. This process helps to preserve the local structure of the data, making it easier to visualize in a 2D or 3D space.
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.spatial</span> <span class="kn">import</span> <span class="n">ConvexHull</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>

<span class="c1"># Assuming &#39;model&#39; and &#39;tokenizer&#39; are predefined</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Accessing the weights of the first layer, which is the Embedding layer</span>

<span class="c1"># Use t-SNE to reduce the dimensionality of the embeddings</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">perplexity</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">tsne_results</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>

<span class="c1"># Apply K-means clustering on the t-SNE results</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">clusters</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">tsne_results</span><span class="p">)</span>

<span class="c1"># Reverse mapping from index to word, adjusted to ensure it matches the embeddings size</span>
<span class="n">num_embeddings</span> <span class="o">=</span> <span class="n">tsne_results</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">reverse_word_map</span> <span class="o">=</span> <span class="p">{</span><span class="n">index</span><span class="p">:</span> <span class="n">word</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">index</span> <span class="o">&lt;</span> <span class="n">num_embeddings</span><span class="p">}</span>

<span class="c1"># Plotting with cluster-based coloring</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="s1">&#39;all&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">scatter</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">tsne_results</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">tsne_results</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">clusters</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Spectral&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;+&quot;</span><span class="p">)</span>

<span class="c1"># Generate a color map from the scatter plot colors</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">(</span><span class="n">scatter</span><span class="o">.</span><span class="n">cmap</span><span class="p">(</span><span class="n">scatter</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">clusters</span><span class="p">))))</span>

<span class="c1"># Annotate a subset of words to avoid clutter</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">):</span>  
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">20</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">reverse_word_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">word</span><span class="p">:</span>
            <span class="n">safe_word</span> <span class="o">=</span> <span class="n">word</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;$&quot;</span><span class="p">,</span> <span class="s2">&quot;\$&quot;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">safe_word</span><span class="p">,</span> <span class="p">(</span><span class="n">tsne_results</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">tsne_results</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="c1"># Draw convex hulls with colors matching their clusters</span>
<span class="k">for</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">kmeans</span><span class="o">.</span><span class="n">n_clusters</span><span class="p">):</span>
    <span class="c1"># Find the points within this cluster</span>
    <span class="n">points_in_cluster</span> <span class="o">=</span> <span class="n">tsne_results</span><span class="p">[</span><span class="n">clusters</span> <span class="o">==</span> <span class="n">cluster</span><span class="p">,</span> <span class="p">:]</span>
    <span class="k">if</span> <span class="n">points_in_cluster</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
        <span class="c1"># Convex hulls require at least 3 points</span>
        <span class="k">continue</span>
    <span class="n">hull</span> <span class="o">=</span> <span class="n">ConvexHull</span><span class="p">(</span><span class="n">points_in_cluster</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">simplex</span> <span class="ow">in</span> <span class="n">hull</span><span class="o">.</span><span class="n">simplices</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">points_in_cluster</span><span class="p">[</span><span class="n">simplex</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points_in_cluster</span><span class="p">[</span><span class="n">simplex</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="o">.</span><span class="n">colors</span><span class="p">[</span><span class="n">cluster</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;t-SNE dimension 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;t-SNE dimension 2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;t-SNE Visualization of Word Embeddings with Cluster-based Coloring and Colored Convex Hulls&#39;</span><span class="p">)</span>

<span class="c1"># Creating a legend for clusters</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="o">*</span><span class="n">scatter</span><span class="o">.</span><span class="n">legend_elements</span><span class="p">(),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;best&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Clusters&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[t-SNE] Computing 31 nearest neighbors...
[t-SNE] Indexed 5000 samples in 0.003s...
[t-SNE] Computed neighbors for 5000 samples in 0.048s...
[t-SNE] Computed conditional probabilities for sample 1000 / 5000
[t-SNE] Computed conditional probabilities for sample 2000 / 5000
[t-SNE] Computed conditional probabilities for sample 3000 / 5000
[t-SNE] Computed conditional probabilities for sample 4000 / 5000
[t-SNE] Computed conditional probabilities for sample 5000 / 5000
[t-SNE] Mean sigma: 0.052459
[t-SNE] KL divergence after 250 iterations with early exaggeration: 72.451553
[t-SNE] KL divergence after 300 iterations: 2.603390
</pre></div>
</div>
<img alt="../_images/f563d6b9d1c2da3166ebab05b7aea5bfb494bd25006eb8d5f6f30a12ade0511b.png" src="../_images/f563d6b9d1c2da3166ebab05b7aea5bfb494bd25006eb8d5f6f30a12ade0511b.png" />
</div>
</div>
</section>
<section id="final-remarks">
<h2>Final remarks<a class="headerlink" href="#final-remarks" title="Link to this heading">#</a></h2>
<p>In this exercise, we have undertaken the task of constructing a basic neural network using TensorFlow and Keras. The process covered essential steps including data preparation, loading, and network training. Despite the educational value of this exercise, we encountered limitations due to the small size of our dataset, which led to observable overfitting and modest accuracy levels.</p>
<p>These challenges emphasize the critical balance between model complexity and the scope and quality of data. Small datasets may not provide the model with sufficient information to learn generalized patterns effectively, resulting in superior performance on training data but poor generalizability to new, unseen data.</p>
<p>This experiment, while highlighting certain limitations, also sets the stage for future exploration. It brings to light the importance of employing strategies to mitigate overfitting, such as incorporating dropout layers, applying regularization techniques, or enhancing the dataset through augmentation. Furthermore, it invites the exploration of more advanced models, hyperparameter optimization, and the acquisition of larger and more varied datasets to improve model performance.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./experiments"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ml-pipeline-sklearn.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">A Wine Quality Prediction Experiment with SKLearn Pipelines</p>
      </div>
    </a>
    <a class="right-next"
       href="shakespeare-embeddings.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">An Exploration of Language Prediction: A Simplified Model for Shakespearean Text Generation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-processing">Pre-processing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#importing-the-data">Importing the data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenizing-content">Tokenizing content</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#splitting-the-dataset-into-training-and-validation">Splitting the dataset into training and validation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-the-classification-model">Defining the classification model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-model-layers">Understanding the model layers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-compilation">Model Compilation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-the-model">Training the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plotting-and-visualizing-the-training-history">Plotting and visualizing the training history</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualising-embeddings">Visualising embeddings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-remarks">Final remarks</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Pedro Leitao
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <div>
<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png"></a>
    All content on this site (unless otherwise specified) is licensed under the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0 license</a>
</div>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>